{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.085525Z",
     "start_time": "2024-11-05T13:50:52.083134Z"
    },
    "id": "dA4DURCWwvWU"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IagZMs0_qjdL"
   },
   "source": [
    "# **Lab 4 : Neural Network**\n",
    "\n",
    "In *lab 4*, you need to finish:\n",
    "\n",
    "1. Basic Part (65%):\n",
    "  Implement a deep neural network from scratch\n",
    "\n",
    "  > * Section 1: Neural network implementation\n",
    "    >> * Part 1: Linear layer\n",
    "    >> * Part 2: Activation function layer\n",
    "    >> * Part 3: Build model\n",
    "\n",
    "  > * Section 2: Loss function\n",
    "    >> * Part 1: Binary cross-entropy loss (BCE)\n",
    "    >> * Part 2: Categorical cross-entropy loss (CCE)\n",
    "    >> * Part 3: Mean square error (MSE)\n",
    "  > * Section 3: Training and prediction\n",
    "    >> * Part 1: Training function & batch function\n",
    "    >> * Part 2: Regression\n",
    "    >> * Part 3: Binary classification\n",
    "\n",
    "\n",
    "2. Advanced Part (30%): Multi class classification\n",
    "3. Report (5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGFR00CQvoaH"
   },
   "source": [
    "## **Important  notice**\n",
    "\n",
    "* Please **do not** change the code outside this code bracket in the basic part.\n",
    "  ```\n",
    "  ### START CODE HERE ###\n",
    "  ...\n",
    "  ### END CODE HERE ###\n",
    "  ```\n",
    "\n",
    "* Please **do not** import any other packages in both basic and advanced part\n",
    "\n",
    "* Please **do not** change the random seed **np.random.seed(1)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BgcgLVV79Bm"
   },
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.108271Z",
     "start_time": "2024-11-05T13:50:52.105534Z"
    },
    "id": "fmTH9UkeqdYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO31dEFx-C1y"
   },
   "source": [
    "### Common Notation\n",
    "  * $C$: number of classes\n",
    "  * $n$: number of samples\n",
    "  * $f^{[l]}$: the dimension of outputs in layer $l$, but $f^{[0]}$ is the input dimension\n",
    "  * $Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}$\n",
    "      * $Z^{[l]}$: the output of layer $l$ in the shape $(n, f^{[l]})$\n",
    "      * $A^{[l]}$: the activation of $Z^{[l]}$ in the shape $(n, f^{[l]})$, but $A^{[0]}$ is input $X$\n",
    "      * $W^{[l]}$: the weight in layer $l$ in the shape $(f^{[l-1]}, f^{[l]})$\n",
    "      * $b^{[l]}$: the bias in layer $l$ in the shape $(1, f^{[l]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wE5z0w8FQLK"
   },
   "source": [
    "# **Basic Part (65%)**\n",
    "In the Basic Part, you will implement a neural network framework capable of handling both regression, binary classification and multi-class classification tasks.\n",
    "\n",
    "**Note:**\n",
    "After implementing each class/function, test it with the provided input variables to verify its correctness. Save the results in the **outputs** dictionary. (The code for testing and saving results is already provided.)\n",
    "## Section 1: Neural network implementation\n",
    "* Part 1: Linear layer\n",
    "> * Step 1: Linear Initialize parameters\n",
    "> * Step 2: Linear forward\n",
    "> * Step 3: Linear backward\n",
    "> * Step 4: Linear update parameters\n",
    "* Part 2: Activation function layer\n",
    "> * Step 1: Activation forward\n",
    "> * Step 2: Activation backward\n",
    "* Part 3: Build model\n",
    "> * Step 1: Model Initialize parameters\n",
    "> * Step 2: Model forward\n",
    "> * Step 3: Model backward\n",
    "> * Step 4: Model update parameters\n",
    "\n",
    "## Section 2: Loss function\n",
    "* Part 1: Binary cross-entropy loss (BCE)\n",
    "* Part 2: Categorical cross-entropy loss (CCE)\n",
    "* Part 3: Mean square error (MSE)\n",
    "\n",
    "## Section 3: Training and prediction\n",
    "* Part 1: Training function & batch function\n",
    "* Part 2: Regression\n",
    "* Part 3: Binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w35ZkTwMc00G"
   },
   "source": [
    "## **Section 1: Neural network implementation(30%)**\n",
    "To implement a neural network, you need to complete 3 classes: **Dense**, **Activation**, and **Model**.\n",
    "The process of training a deep neural network is composed of 3 steps: *forward propagation*, *backward propagation*, and *update*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_krGKUNg_Ix"
   },
   "source": [
    "## Part 1: Linear layer (10%)\n",
    "Dense layer (fully-connected layer) performs linear transformation:\n",
    "\n",
    "$Z = AW + b$, where W is weight matrix and b is bias vector.\n",
    "\n",
    "> ### Step 1: Initialize parameters (0%)\n",
    " * You don't need to write this part.\n",
    " * W is randomly initialized using uniform distribution within $[\\text\\{-limit\\}, \\text\\{limit\\}]$, where $\\text\\{limit\\} = \\sqrt{\\frac{6}{\\text\\{fanin\\} + \\text\\{fanout\\}}}$ (fanin: number of input features, fanout: number of output features)\n",
    " * b is initialized to 0\n",
    "\n",
    "> ### Step 2: Linear forward (4%)\n",
    "* Compute Z using matrix multiplication and addition\n",
    "\n",
    "> ### Step 3: Linear backward (4%)\n",
    "* Use backpropagation to compute gradients of loss function with respect to parameters\n",
    "* For layer l: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by activation)\n",
    "* Given $dZ^{[l]}$ (gradient of loss with respect to Z), we need to compute three gradients:\n",
    "  * $dW^{[l]}$: gradient of loss with respect to weights\n",
    "  * $db^{[l]}$: gradient of loss with respect to bias\n",
    "  * $dA^{[l-1]}$: gradient of loss with respect to previous layer output\n",
    "\n",
    "> Formulas:\n",
    "$$ dW^{[l]} = \\frac{1}{n} A^{[l-1] T} dZ^{[l]} $$\n",
    "$$ db^{[l]} = \\frac{1}{n} \\sum_{i = 1}^{n} dZ_i^{[l]} $$\n",
    "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$\n",
    "\n",
    "> ### Step 4: Linear update parameters (2%)\n",
    "* Update parameters using gradient descent:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.124367Z",
     "start_time": "2024-11-05T13:50:52.118688Z"
    },
    "id": "x0KHo8w9yqbY"
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "  def __init__(self, n_x, n_y, seed=1):\n",
    "    self.n_x = n_x\n",
    "    self.n_y = n_y\n",
    "    self.seed = seed\n",
    "    self.initialize_parameters()\n",
    "\n",
    "  def initialize_parameters(self):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    self.n_x -- size of the input layer\n",
    "    self.n_y -- size of the output layer\n",
    "    self.parameters -- python dictionary containing your parameters:\n",
    "                       W -- weight matrix of shape (n_x, n_y)\n",
    "                       b -- bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "    np.random.seed(self.seed)\n",
    "    W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "    b = np.zeros((1, self.n_y))\n",
    "\n",
    "    assert(W.shape == (self.n_x, self.n_y))\n",
    "    assert(b.shape == (1, self.n_y))\n",
    "\n",
    "    self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "  def forward(self, A):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
    "    self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
    "    \"\"\"\n",
    "\n",
    "    # GRADED FUNCTION: linear_forward\n",
    "    ### START CODE HERE ###\n",
    "    Z = A @ self.parameters[\"W\"] + self.parameters[\"b\"]\n",
    "    self.cache = (A, self.parameters[\"W\"], self.parameters[\"b\"])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "    return Z\n",
    "\n",
    "  def backward(self, dZ):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
    "    self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
    "    self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "\n",
    "    \"\"\"\n",
    "    A_prev, W, b = self.cache\n",
    "    m = A_prev.shape[0]\n",
    "\n",
    "    # GRADED FUNCTION: linear_backward\n",
    "    ### START CODE HERE ###\n",
    "    self.dW = 1/m * A_prev.T @ dZ\n",
    "    self.db = 1/m * np.sum(dZ, axis=0, keepdims=True)\n",
    "    dA_prev = dZ @ W.T\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "    assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "  def update(self, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    learning rate -- step size\n",
    "    \"\"\"\n",
    "\n",
    "    # GRADED FUNCTION: linear_update_parameters\n",
    "    ### START CODE HERE ###\n",
    "    self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "    self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbnVsi6VJMXD"
   },
   "source": [
    "### Test your **Dense class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.143729Z",
     "start_time": "2024-11-05T13:50:52.137409Z"
    },
    "id": "7HNAWwmg8R7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]]\n",
      "b = [[0.]]\n",
      "Z = [[1.9]\n",
      " [2.2]\n",
      " [2.5]]\n",
      "dA_prev = [[3.5]\n",
      " [6. ]]\n",
      "dW = [[1.625 0.625]]\n",
      "db = [[2.   0.75]]\n",
      "W = [[0.5 2.5]]\n",
      "b = [[-1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "dense = Dense(3, 1)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "# Linear forward\n",
    "A, W, b = np.array([[0., 1., 2.], [0.5, 1.5, 2.5], [1., 2., 3.]]), np.array([[0.1], [0.2], [0.3]]), np.array([[1.1]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "print(\"Z = \" + str(Z))\n",
    "\n",
    "A, W, b = np.array([[-0.80,-0.45,-1.11],[-1.65,-2.36,1.14],[-1.02,0.64,-0.86]]), np.array([[0.3], [0.3], [0.1]]), np.array([[-6.2]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "outputs[\"dense_forward\"] = (Z, dense.cache)\n",
    "\n",
    "# Linear backward\n",
    "dZ, linear_cache = np.array([[1.5, 0.5], [2.5, 1.]]), (np.array([[0.5], [1]]), np.array([[2., 1.0]]), np.array([[0.5, 1.]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "print (\"dA_prev = \" + str(dA_prev))\n",
    "print (\"dW = \" + str(dense.dW))\n",
    "print (\"db = \" + str(dense.db))\n",
    "\n",
    "dZ, linear_cache = np.array([[0.52,0.34],[0.76,0.89]]), (np.array([[0.42], [0.68]]), np.array([[0.35, 0.89]]), np.array([[0.12, 0.76]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "outputs[\"dense_backward\"] = (dA_prev, dense.dW, dense.db)\n",
    "\n",
    "# Linear update parameters\n",
    "np.random.seed(1)\n",
    "dense = Dense(1, 2)\n",
    "dense.parameters = {\"W\": np.array([[1.0, 2.0]]), \"b\": np.array([[0.5, 0.5]])}\n",
    "dense.dW = np.array([[0.5, -0.5]])\n",
    "dense.db = np.array([[1.5, -1.5]])\n",
    "dense.update(1.0)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "dense = Dense(3, 4)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(1,4)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(1,4)}\n",
    "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "dense.dW = grads[\"dW1\"]\n",
    "dense.db = grads[\"db1\"]\n",
    "dense.update(0.1)\n",
    "outputs[\"dense_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPtH0j3BFN7"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[-0.20325375]  [0.53968259 [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Z: </td>\n",
    "    <td>[[1.9] [2.2] [2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev: </td>\n",
    "    <td>[[3.5] [6.0]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW: </td>\n",
    "    <td>[[1.625 0.625]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db: </td>\n",
    "    <td>[[2.0 0.75]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[0.5 2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[-1.  2.]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r5m2W3aXh_A"
   },
   "source": [
    "## Part 2: Activation function layer (10%)\n",
    "\n",
    "Implement forward and backward propagation for activation function layers, including Sigmoid, Softmax, and ReLU.\n",
    "\n",
    "> ### Step 1: Forward Propagation (5%)\n",
    " Implement the following activation functions:\n",
    ">> #### a) Sigmoid\n",
    "- Use the numerically stable version to prevent exponential overflow:\n",
    "  $$\\sigma(Z) = \\begin{cases}\n",
    "    \\frac{1}{1+e^{-Z}},& \\text{if } Z \\geq 0\\\\\n",
    "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "\n",
    ">> #### b) ReLU\n",
    "- Simple implementation:\n",
    "  $$RELU(Z) = \\max(Z, 0)$$\n",
    "\n",
    ">> #### c) Softmax\n",
    "- Implement using the numerically stable version:\n",
    "  $$\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{C} e^{Z_j-b}}$$\n",
    "  where $b = \\max_{j=1}^{C} Z_j$\n",
    "\n",
    ">> #### d) Linear\n",
    "- You don't need to implement this part\n",
    "\n",
    "> ### Requirements\n",
    "- Each function should return:\n",
    "  1. Activation value \"a\"\n",
    "  2. Cache containing \"z\" for backward propagation\n",
    "\n",
    "> ### Step 2: Backward Propagation (5%)\n",
    "Implement backward functions for:\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- Softmax\n",
    "- linear\n",
    "\n",
    "> ### General Form\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "where $g(.)$ is the activation function\n",
    "\n",
    "> ### Specific Implementations\n",
    "\n",
    ">> #### a) Sigmoid Backward\n",
    "$$\\sigma'(Z^{[l]}) = \\sigma(Z^{[l]}) (1 - \\sigma(Z^{[l]}))$$\n",
    "Use numerically stable sigmoid\n",
    "\n",
    ">> #### b) ReLU Backward\n",
    "$$g'(Z^{[l]}) = \\begin{cases}\n",
    "    1,& \\text{if } Z^{[l]} > 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    ">> #### c) Softmax Backward\n",
    "For the special case of Softmax combined with Categorical Cross-Entropy loss:\n",
    "$$dZ^{[l]} = s - y$$\n",
    "where $s$ is softmax output, $y$ is true label (one-hot vector)\n",
    "\n",
    "Note: This is a simplified form specific to Softmax + CCE loss combination.\n",
    "\n",
    ">> #### d) linear Backward\n",
    "You don't need to implement this part\n",
    "\n",
    "> ### Note\n",
    "For softmax, use the normalized exponential function to prevent overflow, but use the simplified gradient equation for backwards propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.172234Z",
     "start_time": "2024-11-05T13:50:52.167459Z"
    },
    "id": "Nnuv8MmebMgg"
   },
   "outputs": [],
   "source": [
    "class Activation():\n",
    "  def __init__(self, activation_function, loss_function):\n",
    "    self.activation_function = activation_function\n",
    "    self.loss_function = loss_function\n",
    "    self.cache = None\n",
    "\n",
    "  def forward(self, Z):\n",
    "    if self.activation_function == \"sigmoid\":\n",
    "      \"\"\"\n",
    "      Implements the sigmoid activation in numpy\n",
    "\n",
    "      Arguments:\n",
    "      Z -- numpy array of any shape\n",
    "      self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "      Returns:\n",
    "      A -- output of sigmoid(z), same shape as Z\n",
    "      \"\"\"\n",
    "\n",
    "      # GRADED FUNCTION: sigmoid_forward\n",
    "      ### START CODE HERE ###\n",
    "\n",
    "      #A = 1/(1+np.exp(-Z)) if np.all(Z) >= 0 else np.exp(Z)/(1+np.exp(Z)) #not sure\n",
    "      A = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n",
    "      self.cache = Z\n",
    "      ### END CODE HERE ###\n",
    "\n",
    "      return A\n",
    "    elif self.activation_function == \"relu\":\n",
    "      \"\"\"\n",
    "      Implement the RELU function in numpy\n",
    "      Arguments:\n",
    "      Z -- numpy array of any shape\n",
    "      self.cache -- stores Z as well, useful during backpropagation\n",
    "      Returns:\n",
    "      A -- output of relu(z), same shape as Z\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "      # GRADED FUNCTION: relu_forward\n",
    "      ### START CODE HERE ###\n",
    "      A = np.maximum(0, Z)\n",
    "      self.cache = Z\n",
    "      ### END CODE HERE ###\n",
    "\n",
    "      assert(A.shape == Z.shape)\n",
    "\n",
    "      return A\n",
    "    elif self.activation_function == \"softmax\":\n",
    "      \"\"\"\n",
    "      Implements the softmax activation in numpy\n",
    "\n",
    "      Arguments:\n",
    "      Z -- np.array with shape (n, C)\n",
    "      self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "      Returns:\n",
    "      A -- output of softmax(z), same shape as Z\n",
    "      \"\"\"\n",
    "\n",
    "      # GRADED FUNCTION: softmax_forward\n",
    "      ### START CODE HERE ###\n",
    "      Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "      A = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "      \n",
    "      self.cache = Z\n",
    "      ### END CODE HERE ###\n",
    "\n",
    "      return A\n",
    "    elif self.activation_function == \"linear\":\n",
    "      \"\"\"\n",
    "      Linear activation (returns Z directly).\n",
    "      \"\"\"\n",
    "      self.cache = Z.copy()\n",
    "      return Z\n",
    "\n",
    "    else:\n",
    "      raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "  def backward(self, dA=None, Y=None):\n",
    "    if self.activation_function == \"sigmoid\":\n",
    "      \"\"\"\n",
    "      Implement the backward propagation for a single SIGMOID unit.\n",
    "      Arguments:\n",
    "      dA -- post-activation gradient, of any shape\n",
    "      self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "      Returns:\n",
    "      dZ -- Gradient of the loss with respect to Z\n",
    "      \"\"\"\n",
    "\n",
    "      # GRADED FUNCTION: sigmoid_backward\n",
    "      ### START CODE HERE ###\n",
    "      Z = self.cache\n",
    "      # sigmoid = lambda X:  np.array([1/(1+np.exp(-x)) if x >= 0 else np.exp(x)/(1+np.exp(x)) for x in X])\n",
    "      #sigmoid = lambda X: np.array([1/(1+np.exp(-x)) if x >= 0 else np.exp(x)/(1+np.exp(x)) for x in X])\n",
    "      A  = self.forward(Z)\n",
    "      dZ = dA * A *(1-A)\n",
    "      ### END CODE HERE ###\n",
    "        \n",
    "      assert (dZ.shape == Z.shape)\n",
    "      return dZ\n",
    "\n",
    "    elif self.activation_function == \"relu\":\n",
    "      \"\"\"\n",
    "      Implement the backward propagation for a single RELU unit.\n",
    "      Arguments:\n",
    "      dA -- post-activation gradient, of any shape\n",
    "      self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "      Returns:\n",
    "      dZ -- Gradient of the loss with respect to Z\n",
    "      \"\"\"\n",
    "\n",
    "      # GRADED FUNCTION: relu_backward\n",
    "      ### START CODE HERE ###\n",
    "      Z = self.cache\n",
    "      dZ = np.array(dA, copy=True)\n",
    "      dZ = dA * (Z>0)\n",
    "      ### END CODE HERE ###\n",
    "\n",
    "      assert (dZ.shape == Z.shape)\n",
    "\n",
    "      return dZ\n",
    "\n",
    "    elif self.activation_function == \"softmax\":\n",
    "      \"\"\"\n",
    "      Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
    "      Arguments:\n",
    "      Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                in a Rock-Paper-Scissors, shape: (n, C)\n",
    "      self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "      Returns:\n",
    "      dZ -- Gradient of the cost with respect to Z\n",
    "      \"\"\"\n",
    "\n",
    "      # GRADED FUNCTION: softmax_backward\n",
    "      ### START CODE HERE ###\n",
    "      Z = self.cache\n",
    "      s = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "      dZ = s - Y\n",
    "      ### END CODE HERE ###\n",
    "\n",
    "      assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "      return dZ\n",
    "\n",
    "    elif self.activation_function == \"linear\":\n",
    "      \"\"\"\n",
    "      Backward propagation for linear activation.\n",
    "      \"\"\"\n",
    "      return dA\n",
    "\n",
    "    else:\n",
    "      raise ValueError(f\"Unsupported activation function: {self.activation_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYVMMS2ecCx"
   },
   "source": [
    "### Test your **Activation class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.190784Z",
     "start_time": "2024-11-05T13:50:52.183336Z"
    },
    "id": "gBuRAoeUC5jV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: A = [[0.00669285]\n",
      " [0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.99330715]]\n",
      "ReLU: A = [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "Softmax: A = \n",
      "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
      "Linear: A = \n",
      "[[ 1  2  3  4]\n",
      " [ 0  1  0  0]\n",
      " [-2 -1  0  1]]\n",
      "Sigmoid: dZ = [[-0.5       ]\n",
      " [-0.26935835]\n",
      " [-0.11969269]\n",
      " [-0.5       ]\n",
      " [-0.73139639]]\n",
      "ReLU: dZ = [[-0.    1.7 ]\n",
      " [-0.    0.  ]\n",
      " [-1.14  3.72]]\n",
      "Softmax: dZ = [[-0.96488097  0.70538451  0.25949646]\n",
      " [ 0.09003057 -0.75527153  0.66524096]\n",
      " [ 0.01766842  0.01766842 -0.03533684]]\n",
      "Linear: dZ = \n",
      "[[ 1.2 -0.5  0.8 -0.3]\n",
      " [ 0.4  0.6 -0.9  0.2]\n",
      " [-0.1  0.5 -0.7  0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Activation forward\n",
    "Z = np.array([[-5], [-1], [0], [1], [5]])\n",
    "\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "A = sigmoid.forward(Z)\n",
    "print(\"Sigmoid: A = \" + str(A))\n",
    "A = sigmoid.forward(np.array([[0.23], [-0.67], [0.45], [0.89], [-0.10]]))\n",
    "outputs[\"sigmoid\"] = (A, sigmoid.cache)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "A = relu.forward(Z)\n",
    "print(\"ReLU: A = \" + str(A))\n",
    "A = relu.forward(np.array([[-0.34], [-0.76], [0.21], [-0.98], [0.54]]))\n",
    "outputs[\"relu\"] = (A, relu.cache)\n",
    "\n",
    "Z = np.array([[1, 2, 3, 4],[0, 1, 0, 0],[-2, -1, 0, 1]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "A = softmax.forward(Z)\n",
    "print(\"Softmax: A = \\n\" + str(A))\n",
    "A = softmax.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"softmax\"] = (A, softmax.cache)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "A = linear.forward(Z)\n",
    "print(\"Linear: A = \\n\" + str(A))\n",
    "A = linear.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"linear\"] = (A, Z)  # For linear activation, cache is just Z\n",
    "\n",
    "# Activation backward\n",
    "dA, cache = np.array([[-2], [-1.37], [-1.14], [-2], [-3.72]]), np.array([[0], [1], [2], [0], [1]])\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "sigmoid.cache = cache\n",
    "dZ = sigmoid.backward(dA=dA)\n",
    "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[9.73], [-7.56], [8.34], [-4.12], [6.89]]), np.array([[-5.45], [3.68], [-2.32], [4.51], [-9.27]])\n",
    "sigmoid.cache = cache\n",
    "outputs[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "dA, cache = np.array([[-2., 1.7 ], [-1.37, 2.], [-1.14, 3.72]]), np.array([[-2, 1], [-1, 0], [2, 1]])\n",
    "relu.cache = cache\n",
    "dZ = relu.backward(dA=dA)\n",
    "print(\"ReLU: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[7.24, -3.58], [8.93, 6.45], [-2.11, 9.87]]), np.array([[-4.76, 5.34], [1.98, -7.22], [3.67, -8.56]])\n",
    "relu.cache = cache\n",
    "outputs[\"relu_backward\"] = relu.backward(dA=dA)\n",
    "\n",
    "Y, cache = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "softmax.cache = cache\n",
    "dZ = softmax.backward(Y=Y)\n",
    "print(\"Softmax: dZ = \" + str(dZ))\n",
    "Y, cache = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]]), np.array([[-9.45, 7.32, 3.58], [5.61, -8.27, 6.49], [1.23, -4.56, 7.84]])\n",
    "softmax.cache = cache\n",
    "outputs[\"softmax_backward\"] = softmax.backward(Y=Y)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "dA = np.array([[1.2, -0.5, 0.8, -0.3], [0.4, 0.6, -0.9, 0.2], [-0.1, 0.5, -0.7, 0.9]])\n",
    "dZ = linear.backward(dA=dA)\n",
    "print(\"Linear: dZ = \\n\" + str(dZ))\n",
    "outputs[\"linear_backward\"] = dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyyX_xxdEmNp"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Sigmoid: A</td>\n",
    "    <td>[[0.00669285] [0.26894142] [0.5] [0.73105858] [0.99330715]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReLU: A</td>\n",
    "    <td>[[0] [0] [0] [1] [5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Softmax: A</td>\n",
    "    <td>\n",
    "      [[0.0320586 0.08714432 0.23688282 0.64391426]\n",
    "       [0.1748777 0.47536689 0.1748777 0.1748777]\n",
    "       [0.0320586 0.08714432 0.23688282 0.64391426]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Linear: A</td>\n",
    "    <td>\n",
    "      [[1 2 3 4]\n",
    "       [0 1 0 0]\n",
    "       [-2 -1 0 1]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Sigmoid) dZ</td>\n",
    "    <td>[[-0.5] [-0.26935835] [-0.11969269] [-0.5] [-0.73139639]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with ReLU) dZ</td>\n",
    "    <td>[[0 1.7] [0 0] [-1.14 3.72]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Softmax) dZ</td>\n",
    "    <td>\n",
    "      [[-0.96488097 0.70538451 0.25949646]\n",
    "       [0.09003057 -0.75527153 0.66524096]\n",
    "       [0.01766842 0.01766842 -0.03533684]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Linear) dZ</td>\n",
    "    <td>\n",
    "      [[1.2 -0.5 0.8 -0.3]\n",
    "       [0.4 0.6 -0.9 0.2]\n",
    "       [-0.1 0.5 -0.7 0.9]]\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vcTYp_yoPu"
   },
   "source": [
    "## Part 3: Model (10%)\n",
    "\n",
    "Use the functions that you had previously written to implement the complete neural network model, including initialization, forward propagation, backward propagation, and parameter updates.\n",
    "\n",
    "> ### Step 1: Model Initialization (0%)\n",
    "Initialize the model by creating linear and activation function layers.\n",
    "\n",
    ">> #### Requirements:\n",
    "- Store linear layers in a list called `linear`\n",
    "- Store activation function layers in a list called `activation`\n",
    "- Use iteration number as seed for each Dense layer initialization\n",
    "\n",
    ">> #### Note:\n",
    "A linear-activation pair counts as a single layer in the neural network.\n",
    "\n",
    "> ### Step 2: Forward Propagation (4%)\n",
    "Implement the model's forward pass by calling each layer's forward function sequentially.\n",
    "\n",
    ">> #### Process:\n",
    "1. For layers 1 to N-1: [LINEAR -> ACTIVATION]\n",
    "2. Final layer: LINEAR -> SIGMOID (binary) or SOFTMAX (multi-class)\n",
    "\n",
    ">> #### Note:\n",
    "For binary classification, use one output node with sigmoid activation. For K-class classification, use K output nodes with softmax activation.\n",
    "\n",
    "> ### Step 3: Backward Propagation (4%)\n",
    "Implement the model's backward pass by calling each layer's backward function in reverse order.\n",
    "\n",
    ">> #### Process:\n",
    "1. Initialize backpropagation:\n",
    "   - Regression:\n",
    "     $$dAL = AL - Y$$\n",
    "   - Binary classification:\n",
    "     $$dAL = - (\\frac{Y}{AL + \\epsilon} - \\frac{1 - Y}{1 - AL + \\epsilon})$$\n",
    "     where $\\epsilon = 10^{-5}$ to prevent division by zero\n",
    "   - Multi-class classification:\n",
    "     Use `softmax_backward` function\n",
    "2. Backpropagate through layers L to 1\n",
    "\n",
    ">> #### Note:\n",
    "Use cached values from the forward pass in each layer's backward function.\n",
    "\n",
    "> ### Step 4: Parameter Update (2%)\n",
    "Update model parameters using gradient descent.\n",
    "\n",
    ">> #### Update Rule:\n",
    "For each layer $l = 1, 2, ..., L$:\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
    "where $\\alpha$ is the learning rate\n",
    "\n",
    "This revised structure provides a clear, step-by-step breakdown of the model implementation process, mirroring the format used in Part 2. It covers all the essential components while maintaining a concise and logical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.313801Z",
     "start_time": "2024-11-05T13:50:52.306910Z"
    },
    "id": "0JGMzfIDCSVz"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "  def __init__(self, units, activation_functions, loss_function):\n",
    "    self.units = units\n",
    "    self.activation_functions = activation_functions\n",
    "    self.loss_function = loss_function\n",
    "    self.initialize_parameters()\n",
    "\n",
    "  def initialize_parameters(self):\n",
    "    \"\"\"\n",
    "    Initialize layers of the neural network\n",
    "\n",
    "    Arguments:\n",
    "        self.units -- array defining network structure (e.g., [4,4,1]):\n",
    "            - Input layer: 4 nodes\n",
    "            - Hidden layer: 4 nodes\n",
    "            - Output layer: 1 node\n",
    "        self.activation_functions -- activation function for each layer (e.g., [\"relu\",\"sigmoid\"]):\n",
    "            - First layer uses ReLU\n",
    "            - Second layer uses Sigmoid\n",
    "        self.loss_function -- loss function type: \"cross_entropy\" or \"mse\"\n",
    "    \"\"\"\n",
    "    self.linear = []        # Store all Dense layers (weights & biases)\n",
    "    self.activation = []    # Store all activation function layers\n",
    "\n",
    "    for i in range(len(self.units)-1):\n",
    "      dense = Dense(self.units[i], self.units[i+1], i)\n",
    "      self.linear.append(dense)\n",
    "\n",
    "    for i in range(len(self.activation_functions)):\n",
    "      self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "  def forward(self, X):\n",
    "    \"\"\"\n",
    "    Forward propagation through the network\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data: shape (n, f)\n",
    "    Returns:\n",
    "    A -- model output:\n",
    "        - For binary classification: probability (0-1)\n",
    "        - For multi-class: probability distribution across classes\n",
    "        - For regression: predicted values\n",
    "    \"\"\"\n",
    "    A = X\n",
    "\n",
    "    # GRADED FUNCTION: model_forward\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(len(self.linear)):\n",
    "      A = self.linear[i].forward(A)\n",
    "      A = self.activation[i].forward(A)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return A\n",
    "\n",
    "  def backward(self, AL=None, Y=None):\n",
    "    \"\"\"\n",
    "    Backward propagation to compute gradients\n",
    "\n",
    "    Arguments:\n",
    "        AL -- model output from forward propagation:\n",
    "            - For binary: probability (n,1)\n",
    "            - For multi-class: probabilities (n,C)\n",
    "        Y -- true labels:\n",
    "            - For binary: 0/1 labels (n,1)\n",
    "            - For multi-class: one-hot vectors (n,C)\n",
    "            - For regression: true values (n,1)\n",
    "\n",
    "    Returns:\n",
    "        dA_prev -- gradients for previous layer's activation\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(self.linear)\n",
    "    C = Y.shape[1]\n",
    "\n",
    "    # assertions\n",
    "    warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "    assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "    if self.loss_function == \"cross_entropy\":\n",
    "      if Y.shape[1] == 1:  # binary classification\n",
    "        assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "      else:  # multi-class classification\n",
    "        assert self.activation_functions[-1] == 'softmax', warning\n",
    "        assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "    elif self.loss_function == \"mse\":\n",
    "      assert self.activation_functions[-1] == 'linear', warning\n",
    "      assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "    # GRADED FUNCTION: model_backward\n",
    "    ### START CODE HERE ###\n",
    "    if self.activation_functions[-1] == \"linear\":\n",
    "      # Initializing the backpropagation\n",
    "      dAL = AL - Y\n",
    "      # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "      dZ = self.activation[L-1].backward(dAL, Y)\n",
    "      dA_prev = self.linear[L-1].backward(dZ)\n",
    "\n",
    "    elif self.activation_functions[-1] == \"sigmoid\":\n",
    "      # Initializing the backpropagation\n",
    "      dAL = -(np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))\n",
    "      # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "      dZ = self.activation[L-1].backward(dAL, Y)\n",
    "      dA_prev = self.linear[L-1].backward(dZ)\n",
    "\n",
    "    elif self.activation_functions[-1] == \"softmax\":\n",
    "      # Initializing the backpropagation\n",
    "      dZ = AL - Y\n",
    "\n",
    "      # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "      dA_prev = self.linear[L-1].backward(dZ)\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    # lth layer: (RELU -> LINEAR) gradients.\n",
    "    # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "    for l in range(L-2, -1, -1):\n",
    "      dZ = self.activation[l].backward(dA_prev, Y) \n",
    "      dA_prev = self.linear[l].backward(dZ)\n",
    "    ### END CODE HERE ###\n",
    "    return dA_prev\n",
    "\n",
    "  def update(self, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    learning_rate -- step size\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(self.linear)\n",
    "\n",
    "    # GRADED FUNCTION: model_update_parameters\n",
    "    ### START CODE HERE ###\n",
    "    for l in range(L):\n",
    "      self.linear[l].update(learning_rate)\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxQtZMmA1SNc"
   },
   "source": [
    "### Test your **Model class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.333964Z",
     "start_time": "2024-11-05T13:50:52.325077Z"
    },
    "id": "EGY7_1bjcm-c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:  [[ 0.09762701  0.08976637 -0.12482558]\n",
      " [ 0.43037873 -0.1526904   0.783546  ]\n",
      " [ 0.20552675  0.29178823  0.92732552]] \n",
      "b1:  [[0. 0. 0.]]\n",
      "W2:  [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]] \n",
      "b2:  [[0.]]\n",
      "With sigmoid: A = [[0.64565631]\n",
      " [0.20915937]\n",
      " [0.77902611]]\n",
      "With ReLU: A = [[0.6 ]\n",
      " [0.  ]\n",
      " [1.26]]\n",
      "With softmax: A = \n",
      "[[0.47535001 0.14317267 0.38147732]\n",
      " [0.05272708 0.75380161 0.19347131]\n",
      " [0.68692136 0.05526942 0.25780921]]\n",
      "AL = [[0.56058713]\n",
      " [0.55220559]\n",
      " [0.46331713]]\n",
      "Length of layers list = 2\n",
      "AL = [[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001\n",
      "  0.08448599 0.09067641 0.1294968  0.08303407]\n",
      " [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785\n",
      "  0.08664355 0.09207969 0.12512634 0.08536063]\n",
      " [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299\n",
      "  0.08734059 0.12452515 0.13002144 0.08467232]]\n",
      "Length of layers list = 2\n"
     ]
    }
   ],
   "source": [
    "# Model initialize parameters\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
    "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])\n",
    "\n",
    "# Model forward\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[4.35, -5.67], [-7.89, 8.12]]), np.array([[-3.54], [-2.34]]), np.array([[0.8]])\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[7.23, -4.56], [5.67, -8.90]]), np.array([[-9.12], [3.45]]), np.array([[0.25]])\n",
    "model = Model([2, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1, -0.1, -0.1],[0.2, -0.2, 0.],[0.3, -0.3, 0.1]]), np.array([[-0.5, 0.5, 0.1]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With softmax: A = \\n\" + str(A))\n",
    "A_prev, W, b = np.array([[-5.12, 4.56, 7.89], [8.34, -6.78, 2.45], [3.21, -4.67, 5.98]]), np.array([[6.23, -7.85, 4.56], [-3.21, 9.87, -2.34], [1.23, -5.67, 8.90]]), np.array([[4.12, -6.54, 7.89]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "# binary classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))\n",
    "\n",
    "# multi-class classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 10], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEmggOxtdMnl"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.09762701 0.08976637 -0.12482558] [ 0.43037873 -0.1526904 0.783546 ] [ 0.20552675 0.29178823 0.92732552]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[0. 0. 0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[-0.20325375] [ 0.53968259] [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Sigmoid:</td>\n",
    "    <td>A = [[0.64565631] [0.20915937] [0.77902611]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With ReLU:</td>\n",
    "    <td>A = [[0.6 ] [0. ] [1.26]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Softmax:</td>\n",
    "    <td>A = [[0.47535001 0.14317267 0.38147732] [0.05272708 0.75380161 0.19347131] [0.68692136 0.05526942 0.25780921]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.56058713] [0.55220559] [0.46331713]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001 0.08448599 0.09067641 0.1294968  0.08303407]\n",
    "         [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785 0.08664355 0.09207969 0.12512634 0.08536063]\n",
    "         [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299 0.08734059 0.12452515 0.13002144 0.08467232]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.371543Z",
     "start_time": "2024-11-05T13:50:52.361691Z"
    },
    "id": "HOGsyLXPNGh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.55554938  0.27777469]\n",
      " [ 0.49152369  0.24576184]\n",
      " [-0.41996594 -0.20998297]\n",
      " [-0.55554938 -0.27777469]\n",
      " [-0.39321993 -0.19660997]]\n",
      "dW = [[-0.29446117]\n",
      " [ 0.29446117]]\n",
      "db = [[-0.03216622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[-0.01269296 -0.05595562]\n",
      " [ 0.01470136  0.06480946]\n",
      " [ 0.          0.        ]\n",
      " [-0.07496777 -0.0327431 ]\n",
      " [-0.07151883 -0.03123674]]\n",
      "dW = [[ 0.0178719  -0.17321413]\n",
      " [-0.0178719   0.17321413]]\n",
      "db = [[ 0.00335943 -0.11638953]]\n",
      "\n",
      "Binary classification\n",
      "dW1 = [[-0.06277946  0.26602938 -0.37820327]\n",
      " [ 0.          0.05875647  0.        ]\n",
      " [-0.01569486  0.05181823 -0.09455082]]\n",
      "db1 = [[-0.03138973  0.10363646 -0.18910163]]\n",
      "dA_prev = [[-0.02128713  0.03620889 -0.06919444]\n",
      " [ 0.02675119 -0.04550313  0.08695554]\n",
      " [ 0.08406585 -0.52321654 -0.47247201]]\n",
      "\n",
      "Multi-class classification\n",
      "dW1 = [[ 0.16593371  0.33171007 -0.32297709]\n",
      " [ 0.          0.15006987  0.        ]\n",
      " [ 0.04148343  0.04541005 -0.08074427]]\n",
      "db1 = [[ 0.08296685  0.0908201  -0.16148854]]\n",
      "dA_prev = [[-0.04735391  0.08054785 -0.15392528]\n",
      " [ 0.05429414 -0.09235301  0.1764847 ]\n",
      " [ 0.10229066 -0.30227651 -0.34116033]]\n",
      "\n",
      "Regression\n",
      "dW1 = [[ 0.45352627 -1.49031638  2.73218534]\n",
      " [ 0.          1.09795245  0.        ]\n",
      " [ 0.11338157 -0.64706721  0.68304634]]\n",
      "db1 = [[ 0.22676313 -1.29413441  1.36609267]]\n",
      "dA_prev = [[-0.10931473  0.18594169 -0.35533076]\n",
      " [-0.07704814  0.13105702 -0.25044727]\n",
      " [-0.60730166  3.77977844  3.41319394]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model backward\n",
    "AL, Y, linear_activation_cache = np.array([[0.1], [0.2], [0.5], [0.9], [1.0]]), np.array([[0], [0], [1], [1], [1]]), (((np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[2.0], [1.0]]), np.array([[0.5]])), np.array([[0], [1], [2], [0], [1]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "AL, Y, linear_activation_cache = np.array([[0.35], [0.93], [0.23], [0.72], [0.90]]), np.array([[1], [0], [1], [0], [1]]), (((np.array([[-1, 2], [1, 3], [2, 0], [1, -4], [3, -2]]), np.array([[1.7], [3.2]]), np.array([[0.25]])), np.array([[2], [1], [2], [0], [0]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "X, Y = np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[0], [1], [1], [1], [1]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "X, Y = np.array([[4.56, -3.21], [-7.85, 6.34], [2.45, -8.90], [5.67, 3.12], [-4.78, 7.89]]), np.array([[1], [1], [0], [1], [0]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "# binary classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1], [0], [0]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Binary classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# multi-class classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Multi-class classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# regression - mse\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[2.5], [1.8], [3.2]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"linear\"], \"mse\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Regression\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6xzEk3-NGh6"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Sigmoid</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[ 0.55554938  0.27777469] [ 0.49152369  0.24576184] [-0.41996594 -0.20998297] [-0.55554938 -0.27777469] [-0.39321993 -0.19660997]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[-0.29446117] [ 0.29446117]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[-0.03216622]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">ReLU</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.01269296 -0.05595562] [ 0.01470136  0.06480946] [ 0.  0. ] [-0.07496777 -0.0327431 ] [-0.07151883 -0.03123674]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[ 0.0178719  -0.17321413] [-0.0178719   0.17321413]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[ 0.00335943 -0.11638953]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Binary Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[-0.06277946  0.26602938 -0.37820327] [ 0.  0.05875647  0. ] [-0.01569486  0.05181823 -0.09455082]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[-0.03138973  0.10363646 -0.18910163]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.02128713  0.03620889 -0.06919444] [ 0.02675119 -0.04550313  0.08695554] [ 0.08406585 -0.52321654 -0.47247201]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Multi-class Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.16593371  0.33171007 -0.32297709] [ 0.  0.15006987  0. ] [ 0.04148343  0.04541005 -0.08074427]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.08296685  0.0908201  -0.16148854]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.04735391  0.08054785 -0.15392528] [ 0.05429414 -0.09235301  0.1764847 ] [ 0.10229066 -0.30227651 -0.34116033]]</td>\n",
    "  </tr>\n",
    "  <th colspan=\"2\">Regression</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.45352627 -1.49031638  2.73218534] [ 0.          1.09795245  0.        ] [ 0.11338157 -0.64706721  0.68304634]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.22676313 -1.29413441  1.36609267]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.10931473  0.18594169 -0.35533076] [-0.07704814  0.13105702 -0.25044727] [-0.60730166  3.77977844  3.41319394]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.408945Z",
     "start_time": "2024-11-05T13:50:52.402963Z"
    },
    "id": "qoGA4O8BUCvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.39721186  0.07752363  0.392862  ]\n",
      " [ 0.64025004  0.00469968  0.52183369]\n",
      " [-0.09671178  0.09679955  0.33138026]\n",
      " [ 0.27099015  0.33705631  0.67538482]]\n",
      "b1 = [[ 0.16234149  0.78232848 -0.02592894]]\n",
      "W2 = [[0.6012798 ]\n",
      " [0.38575324]\n",
      " [0.49003974]]\n",
      "b2 = [[0.05692437]]\n"
     ]
    }
   ],
   "source": [
    "# Model update\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.1)\n",
    "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
    "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
    "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
    "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.075)\n",
    "outputs[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t-HfnHZWYIa"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Data Representation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.39721186  0.07752363  0.392862 ] [ 0.64025004  0.00469968  0.52183369] [-0.09671178  0.09679955  0.33138026] [ 0.27099015  0.33705631  0.67538482]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[ 0.16234149  0.78232848 -0.02592894]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[0.6012798 ] [0.38575324] [0.49003974]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.05692437]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSBVaQOSRrk"
   },
   "source": [
    "## **Section 2: Loss function(10%)**\n",
    "In this section, you need to implement the loss function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScdQdj85uC0P"
   },
   "source": [
    "## Part 1: Binary cross-entropy loss (BCE) (5%)\n",
    "Compute the binary cross-entropy loss $L$, using the following formula:  $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+\\right)), where\\ =1e-5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.435123Z",
     "start_time": "2024-11-05T13:50:52.432596Z"
    },
    "id": "MjBT0eYQaY81"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_BCE_loss\n",
    "\n",
    "def compute_BCE_loss(AL, Y):\n",
    "  \"\"\"\n",
    "  Implement the binary cross-entropy loss function using the above formula.\n",
    "\n",
    "  Arguments:\n",
    "  AL -- probability vector corresponding to your label predictions, shape (n, 1)\n",
    "  Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (n, 1)\n",
    "\n",
    "  Returns:\n",
    "  loss -- binary cross-entropy loss\n",
    "  \"\"\"\n",
    "\n",
    "  n = Y.shape[0]\n",
    "\n",
    "  # Compute loss from aL and y.\n",
    "  ### START CODE HERE ### ( 1 line of code)\n",
    "  loss = -1/n * np.sum(Y * np.log(AL + 1e-5) + (1 - Y) * np.log(1 - AL + 1e-5))\n",
    "  ### END CODE HERE ###\n",
    "\n",
    "  loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "  assert(loss.shape == ())\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoV03IzimBEN"
   },
   "source": [
    "### Test your **compute_BCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.451729Z",
     "start_time": "2024-11-05T13:50:52.449309Z"
    },
    "id": "r07sqnIXaaMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.5783820772863568\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.9], [0.6], [0.4], [0.1], [0.2], [0.8]]), np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "print(\"loss = \" + str(compute_BCE_loss(AL, Y)))\n",
    "outputs[\"compute_BCE_loss\"] = compute_BCE_loss(np.array([[0.12], [0.85], [0.47], [0.33], [0.76], [0.58], [0.09], [0.62]]), np.array([[1], [1], [0], [1], [0], [1], [1], [0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iRtgOx_IGPo"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.5783820772863568</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aealRyKbcQzG"
   },
   "source": [
    "## Part 2: Categorical cross-entropy loss (CCE) (5%)\n",
    "Compute the categorical cross-entropy loss $L$, using the following formula: $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+\\right)),\\  = 1e-5$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.472890Z",
     "start_time": "2024-11-05T13:50:52.470863Z"
    },
    "id": "Owx-kTdcfxV5"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_CCE_loss\n",
    "\n",
    "def compute_CCE_loss(AL, Y):\n",
    "  \"\"\"\n",
    "  Implement the categorical cross-entropy loss function using the above formula.\n",
    "\n",
    "  Arguments:\n",
    "  AL -- probability vector corresponding to your label predictions, shape (n, C)\n",
    "  Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                    in a Rock-Paper-Scissors, shape: (n, C)\n",
    "\n",
    "  Returns:\n",
    "  loss -- categorical cross-entropy loss\n",
    "  \"\"\"\n",
    "\n",
    "  n = Y.shape[0]\n",
    "\n",
    "  # Compute loss from aL and y.\n",
    "  ### START CODE HERE ### ( 1 line of code)\n",
    "  loss = -1/n * np.sum(Y * np.log(AL + 1e-5))\n",
    "  ### END CODE HERE ###\n",
    "\n",
    "  loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "  assert(loss.shape == ())\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSOsacYQmNAb"
   },
   "source": [
    "### Test your **compute_CCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.484331Z",
     "start_time": "2024-11-05T13:50:52.481578Z"
    },
    "id": "0YbHVAc7hSh3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4722526144672341\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.8, 0.1, 0.1],[0.6, 0.3, 0.1],[0.4, 0.5, 0.1],[0.1, 0.7, 0.2],[0.2, 0.1, 0.7],[0.4, 0.1, 0.5]]), np.array([[1, 0, 0],[1, 0, 0],[0, 1, 0],[0, 1, 0],[0, 0, 1],[0, 0, 1]])\n",
    "print(\"loss = \" + str(compute_CCE_loss(AL, Y)))\n",
    "outputs[\"compute_CCE_loss\"] = compute_CCE_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VVIBB5Ic-D"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.4722526144672341</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_XIpJtBpiAX"
   },
   "source": [
    "## Part 3: Mean square error (MSE) (0%)\n",
    "You don't need to write this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.512786Z",
     "start_time": "2024-11-05T13:50:52.511064Z"
    },
    "id": "6RHLNGsepygt"
   },
   "outputs": [],
   "source": [
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "  m = Y.shape[0]\n",
    "  loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8_AYhzfqlbg"
   },
   "source": [
    "## **Section 3: Training and prediction(35%)**\n",
    "In this section, you will apply your implemented neural network to regression and binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpFQpiK5eF64"
   },
   "source": [
    "## Helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.534733Z",
     "start_time": "2024-11-05T13:50:52.528207Z"
    },
    "id": "woCqucFUYXe6"
   },
   "outputs": [],
   "source": [
    "def predict(x, y_true, model):\n",
    "  \"\"\"\n",
    "  This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "  Arguments:\n",
    "  x -- data set of examples you would like to label\n",
    "  model -- trained model\n",
    "\n",
    "  Returns:\n",
    "  y_pred -- predictions for the given dataset X\n",
    "  \"\"\"\n",
    "\n",
    "  n = x.shape[0]\n",
    "\n",
    "  # Forward propagation\n",
    "  y_pred = model.forward(x)\n",
    "\n",
    "  # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
    "  # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
    "  # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
    "  if y_pred.shape[-1] == 1:\n",
    "    y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "    if y_true is not None:\n",
    "      y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "  # make y_pred/y_true become one-hot prediction result\n",
    "  # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
    "  # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
    "  if y_true is not None:\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "  if y_true is not None:\n",
    "    # compute accuracy\n",
    "    correct = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "      if yt == yp:\n",
    "        correct += 1\n",
    "    print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "    print(f'f1 score for each class: {f1_scores}')\n",
    "    print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "\n",
    "  return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "  # Create DataFrame with ID, x, and y columns\n",
    "  df = pd.DataFrame({\n",
    "    'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "    'y': predicted_y\n",
    "  })\n",
    "\n",
    "  # Ensure ID is the first column\n",
    "  df = df[['ID', 'y']]\n",
    "\n",
    "  # Save to CSV file\n",
    "  df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "  print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlim(0, 11)\n",
    "  ax.set_ylim(-5, 5)\n",
    "  line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "  ground_truth_x = X_train.flatten()\n",
    "  ground_truth_y = Y_train.flatten()\n",
    "  ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "  # show current epoch on the animation / 100 epoch\n",
    "  epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "  def init():\n",
    "    line.set_data([], [])\n",
    "    epoch_text.set_text('')\n",
    "    return line, epoch_text\n",
    "\n",
    "  def update(frame):\n",
    "    epoch = (frame + 1) * 100\n",
    "    _, predicted_y = history[frame]\n",
    "    predicted_x = X_train.flatten()\n",
    "    line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "    epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "    return line, epoch_text\n",
    "\n",
    "  ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "  # save as gif\n",
    "  ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "  plt.close(fig)\n",
    "  print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "  AL = model.forward(X_train)\n",
    "\n",
    "  predicted_x = X_train.flatten()\n",
    "  predicted_y = AL.flatten()\n",
    "\n",
    "  plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "  ground_truth_x = X_train.flatten()\n",
    "  ground_truth_y = Y_train.flatten()\n",
    "\n",
    "  save_prediction_data(predicted_y)\n",
    "\n",
    "  plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "  plt.ylim(-5, 5)\n",
    "  plt.xlim(0, 11)\n",
    "  plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "  plt.show()\n",
    "  print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgVRVmOYG9FK"
   },
   "source": [
    "## Part1: Training function & batch function (5%)\n",
    "The functions defined in this part will be utilized in the subsequent training parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.548566Z",
     "start_time": "2024-11-05T13:50:52.543678Z"
    },
    "id": "fjOBHI0bGVE7"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "\t\"\"\"\n",
    "\tCreates a list of random minibatches from (X, Y)\n",
    "\n",
    "\tArguments:\n",
    "\tX -- input data, of shape (n, f^{0})\n",
    "\tY -- true \"label\" vector, of shape (n, C)\n",
    "\tmini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "\tReturns:\n",
    "\tmini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "\t\"\"\"\n",
    "\n",
    "\tm = X.shape[0]                  # number of training examples\n",
    "\tmini_batches = []\n",
    "\t### START CODE HERE ###\n",
    "\n",
    "\t# Step 1: Shuffle (X, Y)\n",
    "\tpermutation = list(np.random.permutation(m))\n",
    "\tshuffled_X = X\n",
    "\tshuffled_Y = Y\n",
    "\n",
    "\t# Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "\t# Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "\tnum_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "\tfor k in range(0, num_complete_minibatches):\n",
    "\t\tmini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "\t\tmini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "\t\tmini_batch = (mini_batch_X, mini_batch_Y)\n",
    "\t\tmini_batches.append(mini_batch)\n",
    "\n",
    "\t# For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "\tif m % mini_batch_size != 0:\n",
    "\t\tmini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:, :]\n",
    "\t\tmini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:, :]\n",
    "\t\tmini_batch = (mini_batch_X, mini_batch_Y)\n",
    "\t\tmini_batches.append(mini_batch)\n",
    "\t### END CODE HERE ###\n",
    "\n",
    "\treturn mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "\t\"\"\"\n",
    "\tTrains the model using mini-batch gradient descent\n",
    "\n",
    "\tArguments:\n",
    "\tmodel -- the model to be trained\n",
    "\tX_train -- training set, of shape (num_px * num_px * 3, m_train)\n",
    "\tY_train -- training labels, of shape (1, m_train)\n",
    "\tlearning_rate -- learning rate of the gradient descent update rule\n",
    "\tnum_iterations -- number of iterations of the optimization loop\n",
    "\tbatch_size -- size of a mini batch\n",
    "\tprint_loss -- if True, print the loss every print_freq iterations\n",
    "\tprint_freq -- print frequency\n",
    "\tdecrease_freq -- learning rate decrease frequency\n",
    "\tdecrease_proportion -- learning rate decrease proportion\n",
    "\n",
    "\tReturns:\n",
    "\tmodel -- the trained model\n",
    "\tlosses -- list of losses computed during the optimization\n",
    "\thistory -- list of (X_train, Y_pred) tuples for visualization\n",
    "\t\"\"\"\n",
    "\n",
    "\thistory = []\n",
    "\tlosses = []\n",
    "\n",
    "\tfor i in range(num_iterations):\n",
    "\t\t### START CODE HERE ###\n",
    "\t\t# Define mini batches\n",
    "\t\tif batch_size:\n",
    "\t\t\tmini_batches = random_mini_batches(X_train, Y_train, batch_size)\n",
    "\t\telse:\n",
    "\t\t\t# if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "\t\t\tmini_batches = [(X_train, Y_train)]\n",
    "\n",
    "\t\tepoch_loss = 0\n",
    "\t\tfor batch in mini_batches:\n",
    "\t\t\tX_batch, Y_batch = batch\n",
    "\n",
    "\t\t\t# Forward pass\n",
    "\t\t\tAL = model.forward(X_batch)\n",
    "\n",
    "\t\t\t# Compute loss\n",
    "\t\t\tif model.loss_function == 'cross_entropy':\n",
    "\t\t\t\tif model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "\t\t\t\t\tloss = compute_BCE_loss(AL, Y_batch)\n",
    "\t\t\t\telif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "\t\t\t\t\tloss = compute_CCE_loss(AL, Y_batch)\n",
    "\t\t\telif model.loss_function == 'mse': # Regression\n",
    "\t\t\t\tloss = compute_MSE_loss(AL, Y_batch)\n",
    "\t\t\tepoch_loss += loss\n",
    "\n",
    "\t\t\t# Backward pass\n",
    "\t\t\tmodel.backward(AL, Y_batch)\n",
    "\n",
    "\t\t\t# Update parameters\n",
    "\t\t\tmodel.update(learning_rate)\n",
    "\n",
    "\t\tepoch_loss /= len(mini_batches)\n",
    "\t\tlosses.append(epoch_loss)\n",
    "\t\t### END CODE HERE ###\n",
    "\n",
    "\t\t# Print loss\n",
    "\t\tif print_loss and i % print_freq == 0:\n",
    "\t\t\tprint(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "\t\t# Store history\n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\thistory.append((X_train, model.forward(X_train)))\n",
    "\n",
    "\t\t# Decrease learning rate\n",
    "\t\tif i % decrease_freq == 0 and i > 0:\n",
    "\t\t\tlearning_rate *= decrease_proportion\n",
    "\n",
    "\treturn model, losses, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2D8lubAw8pX"
   },
   "source": [
    "## Part 2: Regression (10%)\n",
    "In this part, Your task is to train a neural network model to approximate the following mathematical function:\n",
    "\n",
    "$$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-ksy7v-Hrrt"
   },
   "source": [
    "> ### Step 1: Data generation\n",
    "Generate the mathematical function :  $$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:50:52.559400Z",
     "start_time": "2024-11-05T13:50:52.557646Z"
    },
    "id": "0yRO6y_FyLPM"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_points=1000):\n",
    "\n",
    "  x = np.linspace(0.01, 11, num_points)\n",
    "  y = np.sin(2 * np.sin(2 * np.sin(2 * np.sin(x))))\n",
    "\n",
    "  return x.reshape(-1, 1), y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC_BxLJJHxHD"
   },
   "source": [
    "> ### Step 2: Train model\n",
    "Implement and train your model using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:51:52.495770Z",
     "start_time": "2024-11-05T13:50:52.568031Z"
    },
    "id": "alsJ4F6eHZ2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "Loss after iteration 0: 0.7806307800207124\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n",
      "[[0.07988679]\n",
      " [0.16696334]\n",
      " [0.25234297]\n",
      " [0.33519126]\n",
      " [0.41473899]\n",
      " [0.49030018]\n",
      " [0.56128679]\n",
      " [0.6272193 ]\n",
      " [0.68773328]\n",
      " [0.74258167]]\n",
      "[[0.7916331 ]\n",
      " [0.83486664]\n",
      " [0.87236344]\n",
      " [0.90429598]\n",
      " [0.93091548]\n",
      " [0.95253836]\n",
      " [0.96953211]\n",
      " [0.98230143]\n",
      " [0.99127492]\n",
      " [0.99689289]]\n",
      "[[-0.9302663 ]\n",
      " [-0.93292563]\n",
      " [-0.93556787]\n",
      " [-0.93817081]\n",
      " [-0.9407153 ]\n",
      " [-0.94318516]\n",
      " [-0.9455669 ]\n",
      " [-0.94784961]\n",
      " [-0.95002469]\n",
      " [-0.9520856 ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(layers_dims, activation_fn, loss_function)\n\u001b[1;32m---> 17\u001b[0m model, losses, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecrease_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecrease_proportion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Plot the loss\u001b[39;00m\n\u001b[0;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[1;32mIn[57], line 98\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, Y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\u001b[0m\n\u001b[0;32m     95\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m    101\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate(learning_rate)\n",
      "Cell \u001b[1;32mIn[28], line 97\u001b[0m, in \u001b[0;36mModel.backward\u001b[1;34m(self, AL, Y)\u001b[0m\n\u001b[0;32m     95\u001b[0m   dAL \u001b[38;5;241m=\u001b[39m AL \u001b[38;5;241m-\u001b[39m Y\n\u001b[0;32m     96\u001b[0m   \u001b[38;5;66;03m# Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m   dZ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m   dA_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(dZ)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_functions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    101\u001b[0m   \u001b[38;5;66;03m# Initializing the backpropagation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 117\u001b[0m, in \u001b[0;36mActivation.backward\u001b[1;34m(self, dA, Y)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# GRADED FUNCTION: relu_backward\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[0;32m    116\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m--> 117\u001b[0m dZ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m dZ \u001b[38;5;241m=\u001b[39m dA \u001b[38;5;241m*\u001b[39m (Z\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "x_train, y_train = generate_data()\n",
    "loss_function = \"mse\"\n",
    "#layers_dims = [1, 256, 256, 256, 256, 256, 256, 1]\n",
    "layers_dims = [1, 16, 16, 16, 16, 16, 1]\n",
    "activation_fn = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"linear\"]\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10000\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 100\n",
    "decrease_proportion = 0.9\n",
    "# You don't necessarily need to use mini_batch in this part\n",
    "batch_size = 10\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQQtLyFgL4WD"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to:\n",
    "> * *Lab4_basic_regression.csv*\n",
    "> * *Lab4_basic_regression.jpg*\n",
    "> * *Lab4_basic_regression.gif*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:51:52.564535Z",
     "start_time": "2024-11-05T13:46:39.788624Z"
    },
    "id": "0uwle3uqL9Em"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_basic_regression.csv'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3OElEQVR4nO3deXxU1f3/8fdkX0gG2RIiYUcT9kBcEFQElyr1B7/6dSlYQRaLog3FBREr1IVoRatCQbAVqlarv6ooVi2LLFqLCBJAVkGWyGKAykzYkpC5vz/Od7JAgAQyOZnJ6/l43EcyNzdzPzPJnPu+5557r8txHEcAAAAWhNkuAAAA1F0EEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWRNgu4HR8Pp92796thIQEuVwu2+UAAIBKcBxH+fn5SklJUVjY6fs8anUQ2b17t1JTU22XAQAAzkJubq6aNWt22mVqdRBJSEiQZF5IYmKi5WoAAEBleL1epaamlmzHT6dWBxH/4ZjExESCCAAAQaYywyoYrAoAAKwhiAAAAGsIIgAAwJpaPUYEAFCzHMfR8ePHVVxcbLsU1HKRkZEKDw8/5+chiAAAJEmFhYXas2ePjhw5YrsUBAGXy6VmzZqpXr165/Q8BBEAgHw+n7Zt26bw8HClpKQoKiqKC0nilBzH0b59+/TDDz+oXbt259QzQhABAKiwsFA+n0+pqamKi4uzXQ6CQOPGjbV9+3YVFRWdUxBhsCoAoMSZLscN+FVXjxn/cQAAwBqCCAAAsIYgAgBAJUycOFFdu3YteTxkyBANGDCgxuvYvn27XC6XcnJyanzdgUAQAQAEtSFDhsjlcsnlcikyMlKtW7fWAw88oMOHDwd0vS+++KJmz55dqWVDLTxUJ86aAQAEvZ/97GeaNWuWioqK9Pnnn2v48OE6fPiwpk+fXm65oqIiRUZGVss63W53tTxPXUePCAAg6EVHRys5OVmpqakaOHCgBg0apDlz5pQcTnn11VfVunVrRUdHy3EceTwe3XXXXWrSpIkSExPVp08frV69utxzPv3000pKSlJCQoKGDRumY8eOlfv5iYdmfD6fnnnmGbVt21bR0dFq3ry5nnrqKUlSq1atJEkZGRlyuVzq3bt3ye/NmjVL6enpiomJUVpamqZNm1ZuPcuXL1dGRoZiYmKUmZmpVatWVeM7Zx89IgCAUzpyRNq4sebXm5YmncvlTGJjY1VUVCRJ2rJli9555x29++67Jde76Nevnxo0aKCPP/5YbrdbM2bMUN++fbV582Y1aNBA77zzjiZMmKA//elPuvzyy/X666/rpZdeUuvWrU+5znHjxumVV17RH//4R/Xq1Ut79uzRxv9985YvX66LL75YCxYsUIcOHRQVFSVJeuWVVzRhwgRNnTpVGRkZWrVqlUaMGKH4+HgNHjxYhw8f1s9//nP16dNHb7zxhrZt26asrKyzf2NqI6cW83g8jiTH4/HYLgUAQtrRo0ed9evXO0ePHi03f+VKx5Fqflq5svK1Dx482Onfv3/J46+++spp2LChc8sttzgTJkxwIiMjnby8vJKfL1y40ElMTHSOHTtW7nnatGnjzJgxw3Ecx+nRo4czcuTIcj+/5JJLnC5dulS4Xq/X60RHRzuvvPJKhTVu27bNkeSsWrWq3PzU1FTnzTffLDfviSeecHr06OE4juPMmDHDadCggXP48OGSn0+fPr3C56ppp/qfcZyqbb/pEQEAnFJamrRypZ31VsVHH32kevXq6fjx4yoqKlL//v01ZcoUTZs2TS1atFDjxo1Lll25cqUOHTqkhg0blnuOo0ePauvWrZKkDRs2aOTIkeV+3qNHDy1atKjC9W/YsEEFBQXq27dvpWvet2+fcnNzNWzYMI0YMaJk/vHjx0vGn2zYsEFdunQpd7XbHj16VHodwYAgAgA4pbg4qVs321Wc2VVXXaXp06crMjJSKSkp5QakxsfHl1vW5/OpadOmWrx48UnPU79+/bNaf2xsbJV/x+fzSTKHZy655JJyP/MfQnIc56zqCSYEEQBA0IuPj1fbtm0rtWy3bt20d+9eRUREqGXLlhUuk56ermXLlumOO+4ombds2bJTPme7du0UGxurhQsXavjw4Sf93D8mpLi4uGReUlKSzj//fH3//fcaNGhQhc/bvn17vf766zp69GhJ2DldHcGIs2YAAHXK1VdfrR49emjAgAH617/+pe3bt+vLL7/Uo48+qhUrVkiSsrKy9Oqrr+rVV1/V5s2bNWHCBK1bt+6UzxkTE6OxY8fqoYce0muvvaatW7dq2bJl+stf/iJJatKkiWJjY/Xpp5/qxx9/lMfjkWQukpadna0XX3xRmzdv1tq1azVr1iw9//zzkqSBAwcqLCxMw4YN0/r16/Xxxx9r8uTJAX6HahZBBABQp7hcLn388ce64oorNHToUF1wwQW67bbbtH37diUlJUmSbr31Vj322GMaO3asunfvrh07dujuu+8+7fP+7ne/0/3336/HHntM6enpuvXWW5WXlydJioiI0EsvvaQZM2YoJSVF/fv3lyQNHz5cf/7znzV79mx16tRJV155pWbPnl1yum+9evU0d+5crV+/XhkZGRo/fryeeeaZAL47Nc/l1OIDUF6vV263Wx6PR4mJibbLAYCQdezYMW3btk2tWrVSTEyM7XIQBE73P1OV7Tc9IgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsqbEgkp2dLZfLpdGjR9fUKgEAQC1XI0Hk66+/1syZM9W5c+eaWB0AAEFh4sSJ6tq1q+0yJElDhgzRgAEDany9AQ8ihw4d0qBBg/TKK6/ovPPOC/TqAAB10N69e5WVlaW2bdsqJiZGSUlJ6tWrl15++WUdOXLEdnlnZeLEiXK5XKedtm/fXuXn3b59u1wul3Jycqq95rMR8CAyatQo9evXT1dffXWgVwUAqIO+//57ZWRkaN68eZo0aZJWrVqlBQsW6Le//a3mzp2rBQsWnPJ3i4qKarDSqnnggQe0Z8+ekqlZs2Z6/PHHy81LTU0tWb6wsNBitWcvoEHk73//u7755htlZ2dXavmCggJ5vd5yEwAAp3PPPfcoIiJCK1as0C233KL09HR16tRJN910k/75z3/qxhtvLFnW5XLp5ZdfVv/+/RUfH68nn3xSkjR9+nS1adNGUVFRuvDCC/X666+X/E5FPQgHDx6Uy+XS4sWLJUmLFy+Wy+XSwoULlZmZqbi4OF122WXatGlTuVqffvppJSUlKSEhQcOGDdOxY8dO+brq1aun5OTkkik8PFwJCQkljx9++GHddNNNys7OVkpKii644IKS1zhnzpxyz1W/fn3Nnj1bkkpuqJeRkSGXy6XevXuXW3by5Mlq2rSpGjZsqFGjRgU8rAUsiOTm5iorK0tvvPFGpW+glJ2dLbfbXTKVTXoAAJzowIEDmjdvnkaNGqX4+PgKl3G5XOUeT5gwQf3799fatWs1dOhQvf/++8rKytL999+vb7/9Vr/+9a915513atGiRVWuZ/z48Xruuee0YsUKRUREaOjQoSU/e+eddzRhwgQ99dRTWrFihZo2bapp06ZVeR1lLVy4UBs2bND8+fP10UcfVep3li9fLklasGCB9uzZo/fee6/kZ4sWLdLWrVu1aNEi/fWvf9Xs2bNLAkygRATqiVeuXKm8vDx17969ZF5xcbGWLl2qqVOnqqCgQOHh4eV+Z9y4cRozZkzJY6/XSxgBAJuOHJE2bqz59aalSXFxZ1xsy5YtchxHF154Ybn5jRo1KultGDVqlJ555pmSnw0cOLBcQBg4cKCGDBmie+65R5I0ZswYLVu2TJMnT9ZVV11VpbKfeuopXXnllZKkhx9+WP369dOxY8cUExOjF154QUOHDtXw4cMlSU8++aQWLFhw2l6RM4mPj9ef//xnRUVFVfp3GjduLElq2LChkpOTy/3svPPO09SpUxUeHq60tDT169dPCxcu1IgRI866xjMJWBDp27ev1q5dW27enXfeqbS0NI0dO/akECJJ0dHRio6ODlRJAICq2rhRKrNDWWNWrpS6dav04if2eixfvlw+n0+DBg1SQUFBuZ9lZmaWe7xhwwbddddd5eb17NlTL774YhWLVrmzQ5s2bSpJysvLU/PmzbVhwwaNHDmy3PI9evQ4q54Xv06dOlUphJxJhw4dym2fmzZtetK2vLoFLIgkJCSoY8eO5ebFx8erYcOGJ80HANRSaWkmFNhYbyW0bdtWLpdLG0/otWndurUkKTY29qTfqegQzolBxnGcknlhYWEl8/xONW4iMjLypOf0+XxnfB1n61SvpWytUuUH5Zat3/9cgaxfCmAQAQCEgLi4KvVM1LSGDRvqmmuu0dSpU3XfffedcpzI6aSnp+uLL77QHXfcUTLvyy+/VHp6uqTSQxl79uxRRkaGJJ3Vqa/p6elatmxZufUsW7asys9zJo0bN9aePXtKHn/33XflTmH296AUFxdX+7rPRo0GEf/oYgAAqsu0adPUs2dPZWZmauLEiercubPCwsL09ddfa+PGjeXGKlbkwQcf1C233KJu3bqpb9++mjt3rt57772S035jY2N16aWX6umnn1bLli21f/9+Pfroo1WuMysrS4MHD1ZmZqZ69eqlv/3tb1q3bl1J70116dOnj6ZOnapLL71UPp9PY8eOLdfT0aRJE8XGxurTTz9Vs2bNFBMTI7fbXa01VAX3mgEABLU2bdpo1apVuvrqqzVu3Dh16dJFmZmZmjJlih544AE98cQTp/39AQMG6MUXX9Szzz6rDh06aMaMGZo1a1a501pfffVVFRUVKTMzU1lZWSWn/VbFrbfeqscee0xjx45V9+7dtWPHDt19991Vfp4zee6555SamqorrrhCAwcO1AMPPKC4MgN/IyIi9NJLL2nGjBlKSUlR//79q72GqnA5Jx5IqkW8Xq/cbrc8Ho8SExNtlwMAIevYsWPatm2bWrVqVelLLqBuO93/TFW23/SIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAgBK1+ERK1DLV9b9CEAEAlFzwquwVOIHTKSwslKQK7x1XFVziHQCg8PBw1a9fX3l5eZKkuLi4k+6/Avj5fD7t27dPcXFxiog4tyhBEAEASFLJLeH9YQQ4nbCwMDVv3vycAytBBAAgydxptWnTpmrSpEml79aKuisqKqrkzsTngiACACgnPDz8nI/7A5XFYFUAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFgT0CCSnZ2tiy66SAkJCWrSpIkGDBigTZs2BXKVAAAgiAQ0iCxZskSjRo3SsmXLNH/+fB0/flzXXnutDh8+HMjVAgCAIOFyHMepqZXt27dPTZo00ZIlS3TFFVeccXmv1yu32y2Px6PExMQaqBAAAJyrqmy/a3SMiMfjkSQ1aNCgJlcLAABqqYiaWpHjOBozZox69eqljh07VrhMQUGBCgoKSh57vd6aKg8AAFhQYz0i9957r9asWaO33nrrlMtkZ2fL7XaXTKmpqTVVHgAAsKBGxojcd999mjNnjpYuXapWrVqdcrmKekRSU1MZIwIAQBCpyhiRgB6acRxH9913n95//30tXrz4tCFEkqKjoxUdHR3IkgAAQC0S0CAyatQovfnmm/rggw+UkJCgvXv3SpLcbrdiY2MDuWoAABAEAnpoxuVyVTh/1qxZGjJkyBl/n9N3AQAIPrXq0AwAAMCpcK8ZAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGBNhO0CqoXPJ+3aJYWHS02bSi6X7YrOmuNIR49Kx46ZqaCg9PuyjyMipORk83IbNAjql1w5eXnS4cNSixZSGPkZFv30k/Tf/5r/xYjQaEJRxqFD0t690vnnS7GxtqupE4L/U/Sf/0h33SV9+615fMEF0j33SMOGSfXqBW69jiMVFprvw8JMEggLO2kjWVws/fCDtH9/5aYDB6SioqqVEhUlNWtmptTU0u/LPm7c+By33z5f6eQ4ZoqKCmwoOH5ceust6bnnpNWrzbyWLaUpU6Sf/zxw6wUqcuCAaVveecc8drulwYOlrCypdevArddxTKPg85W2NS6X2fEK1T0Qxynf5oSHBz70ffWVNGmS9Mkn5v2Oj5d+8xvpiSfM+hEwLsdxHNtFnIrX65Xb7ZbH41FiYuLJC6xeLV1+uZSeLj3yiNnqv/OO9O67ppH47W+le+8131fF4cMm2KxZI33/vUkSubnSnj1Sfr5JzIcOmQ/LCZzISBVGJehwWD0dPJ6g/QX15PElyCN3yXQ4wi1fPbecRLdc9d2KaOhWVGO3YpPdik9xKybJrejEaMXESNHRUkyMTvq+sNCE9j17TGfQrl2mxB9+KJ38gSZCRWoQka92SV61TfKq1XkeNXd7lBLvUVLMQTWK8Og810HFFXnk8nqkgwclT5mvHo905EjF71V8vJSQYEJfUpJJPqmpJjB07GimBg2q9v4XFEivvy49/bS0dasJHbffbtYxbZr0r39J8+ZJffpU7XmBs3X0qNS3r7R5s/TkkyZ4LFok/fnPpodk0CBp/HizI1QVx49L331n2ppNm8yHODfXfKA9ntL25vjxk383LMx8JurVK/0MJiRIiYmmzXO7y39fdio7v1696tuh8PlMvf52w+st/f7gwfJty6m+P1VbExNT+nobNizd00pNldq3lzp1kpo3r1o4cxzpiy+kp54y7Upamgmb7dtLn31m2qDRo83OEKrkjNvvMoI3iPh8UmamCR9ffGE+gH47d0rPPGMaibg4afhw6aabpG7dzF683/HjZkP37belwWPNGjPPccyH0/+PnpoqpaSYD3C9enLqJejAkVh9v03atsWn7d/7tHO7T86xY0pQvpo3OKTWjfJ1fv1DahzlVfxxj6ILPIo84lGY9wwbd8mkjrINR2SkSeUnTpJJJWWnoiI5BQXyefLl8noVVnD0lKspVKQ8cuug6is/zK3CGLeKE+sr7Dy3ohq5FZtSX4nN3Epul6CI6PDSvTGXywSG/PzSae/e0oY0N7c0CaWkmEaic+fSr2lp5jWW/Vt8+630/vvm77Znj/R//6/06KNSRkbpcsXF0tVXS9u3S2vXBrbXC/CbONFslD7/XLrootL5R49Kr7xi2pu9e007M2iQdOWVUv36pcs5jvmfXrfOTP62Zt06c6xVkpo0KW1rmjUzv+8PGPHxpj3y9xQ4jvn8+XeK/JPXW7rxLxsCvF7zexVxuUqDSWKiWWdEhFlfeHhpT294uPmcFhaadfvbG//xYn9wOtUmJSzMvKb69c26yn4t+31cXGn75l93UZHZQfQHs337SncQd+ww8yVTe8eO5duaTp1O/lvk5kqffirNnm161Tt2lH73O/P3K9v78fzz0v33S0uWSFdcUcl/Fkh1JYi88Yb0q1+ZENKzZ8VPsHu39OyzZu/6wAHz4Tr/fJOsPR45+/fL9b97GsfrN1R+6y76qVln/ZjUWT807Kyd8e11sCBWhYXm81dcXJpdVqwwh1Ik85QXX2zap4suMvmo7P/9KRUVlW8sTjf5u2aLi8tPkglX/ikysvT7hITSxqXslJAg1a8vJ9Gt/YdjlfuDSzt3Stu2nTz5s1JSkvTqq9INN1Tyj1dUZPYe164tP23fbn4eHi41amTqKygwjXhBganv5pulBx4wYaUiW7eaPZYnn5QefLCSBQFn6ccfpTZtpFGjTOCoyLFj5gMybZoJF5IZxOV2m7By4IDZkEqm/enQQerSxWwou3QxG8uGDQP3GhyntKeiojan7Lz8fNO2+A+L+L8vLjaf2+ho076U/RoTc+qemMRE0yDGxwfmUJLjmFByYluzYUPpzlD9+ub9dblMw33woAk4ffqYHo8bbqi4Np9P6tHDvM7PP6/+2kNYyAWRWbM8io1NLBma4DjSdY9m6lhikhY98E85jmkHjh4tP+Xnm8/XoYPHlbLra6XuX6XEQ7sVVnBUeYVu/VjcSBuVpnXqoDw1kVT6jxgeXtL5oagok2H8IT011YSN7t3N15QUe+9RIDmO2fHYssUcOv3nP6WZM6URI87hSb3e0h6ovDzTIERHm73Brl2lSy8t31NyKkOGmK7xrVsZMIjAeuIJE0Byc6Xzzjvz8t99J339tfnq9Zo9/PPOM2GmQwepVSvGHNSEwkKzM7RmjQkqBw6Y+fXrm79Djx5m8NyZzJljeme/+srscaJSQi6ISB5JpS+ks1Zrtbqqv+boQ/UvmR8TYwY5+6cTOwKqMsXGhu44sLPhOGbc1tSp0p/+ZA6jWrVqlTnU9sEH0v/5P5aLQcjy+aR27Uy3/KxZtquBDcXFZuzPlVeaXi9USlWCSFDsSu7caXr4/EMToh95Tb63k/TW5hvkivrfedEEh0ByuaSXXjJHfkaNMj2eWVkWC8rIMIdn3n2XIILA+fe/zYB1QkjdFR4u3Xab9PLLpuGLjLRdUcgJigsylB1DVa+eFPnpXIUN6K84d6RiY01PCCEk8FwuM3j8oYfMYdXJky0X9ItfSB9+WPXznYHK+vBDM9ajVy/blcCm//kfc+2YxYttVxKSgiKIlPPdd2bq1892JXWSy2VOHnj0UTNONDvbYjE33WTGmCxdarEIhLRPPpGuv56L6NV1XbuaSxJ88IHtSkJS8H26Pv7YjB7lGhLWuFxm/N7vf28u3/L445YK6dLFDHL97DNLBSCk5eaaM2Cuv952JbDN5TKXDVi0yHYlISkoxoiUs2iROV2X60dY99hj5oSV8ePN0ZHHH6/hQ2Qul9S7N92lCIwFC0o3QECfPuYaR3v3msN1qDbB1SPiOGbwGMdra41HHjGXannySWncuFNfyyhgeveWli8310gAqtO//22u71GZU3YR+q66ynxlx6faBVcQ2bzZXIyGIFKrPPCA9MIL5lIL999fw2Gkd29zlbn//KcGV4o64csvpcsus10FaovkZOnCC7mwWQAEVxD54gszaOzSS21XghNkZZnri/zxj+Z6IzUWRi680JxWtXx5Da0QdcJ//2uuzHmqqzajbrrkEnOxOlSr4BojsmyZuSfAGS6OAjvuucecYv/rX5sxI9Om1cDJBmFh5vK2NA6oTsuWma/0iKCsiy82dwQvKKjcFaBRKcHVI5KTY66miVprxAhz8UH/peD9t8MJqIsuIoigen3zjRkb0qqV7UpQm1x8sdnLWr3adiUhJXiCSFGRuZFR1662K8EZDBkivfaaubHlnXfWQBi5+GJzg8PduwO8ItQZOTmmreFKiSirc2fT7cuh4GoVPIdmNm0y3WFlbwmPWuv2282pvbffbsaSvvZaAO9Nl5lpvq5cGbp3IETNysnh1gE4WXS0OZMqJ8d2JSEleHpEVq0yX7t0sVsHKu2226S335b+3/+TBg4M4JXYmzUzd9RcuzZAK0Cd4vWauzrT+4qKdOpk7uiLahM8QSQnR2rd2pwhgaBx003SP/5h7qR9663mztzVzuUyjQNBBNXBv5EhiKAinTqZK+76fLYrCRnBE0RWraJhCFL9+0vvvy/9858mmBQUBGAlBBFUl5wccxuJtDTblaA26txZOnLE3JUZ1SI4gojjmMaB8SFBq18/cyPTBQukAQOko0ereQWdOplxRAHpckGdkpMjdehgwghwok6dzFcOz1Sb4Agie/ZIP/1kkiiC1nXXSR99JC1ZYsYBHjlSjU/eqZMZFbtxYzU+Keqk1asZi4ZTS0qSGjWiB7YaBUcQ2bTJfE1Pt1sHzlnfvubO6v/5j+klqbZbxHTsaL7SOOBcOI65omqHDrYrQW3lcpmdYnpEqk3wBJGoKC4uFCKuvFL617/M2bbXXy/l51fDk7rdUvPmBBGcmx9+kA4fZnwITo8xadUqOILI5s3SBRcE8EIUqGk9e0rz5pmdiuuukzyeanhSTqvDufIf2qP3FafTqZO0ZUsABrvVTcERRDZtomEIQZdeKi1caHrCr7nGDAM6Jx07SuvXV0ttqKM2bDAXrWrZ0nYlqM3S0sxhvO++s11JSCCIwKrMTOmzz8yZcFdfLR04cA5Plp4u7dhhutaBs7Fxo+l9DQ+3XQlqM/+hO//4RZyT4Agi+/YRREJYRoYJI7m5Up8+5s99Vtq3N185cwZna8MGxofgzBo2NGfO0NZUi+AIIhJBJMR17iwtXiz9+KN01VXma5X5NyAcnsHZ2riRtgaVk5ZGEKkmwRNELrjAdgUIsPbtTRj573+l3r3N5WOqJCFBSk0liODsHDwo7d1LjwgqhyBSbYIjiLRsKcXG2q4CNSAtzVzw7NAhc5pvbm4Vn6B9e9O9DlQVZ8ygKi680IwRcRzblQS94Agi9IbUKe3amTBSWCj16lXF8WDt29MjgrOzcaO5WBXtDSojLc0MjN+1y3YlQa9Ggsi0adPUqlUrxcTEqHv37vr888+r9gQXXhiYwlBrtW4t/fvfUr16JoysXFnJX2zf3tzC/dixgNaHELRhg9SihRQXZ7sSBAP/ITwOz5yzgAeRt99+W6NHj9b48eO1atUqXX755br++uu1c+fOyj8JQaROOv986fPPpbZtzZiRRYsq8Uvp6eb23Jzfj6rauJHxIai8li3NFb8JIucs4EHk+eef17BhwzR8+HClp6frhRdeUGpqqqZPn175J/nZzwJXIGq1Bg3MHXt79jT/Bu+9d4Zf8B/f5/AMqmrDBsaHoPIiIsxxZILIOQvoNdMLCwu1cuVKPfzww+XmX3vttfryyy9PWr6goEAFBQUlj71er/mmceNAlolaLj5e+vBD6Y47pJtvlp591gSTijVQRsNk5c1fr10ta7BIBDVXYYEu+v57bYtO076vbFeDYNGu8YUK/2qjNvI/c5KqXFcyoEFk//79Ki4uVlJSUrn5SUlJ2rt370nLZ2dn6/e//30gS0KQioqS/vY3cwfu++8//bIL1F7//ct63fKXmqkNwa+9tmidijXk6TR9/rTtahAsnlSa7tBruvRS25UEtxq5i5zL5Sr32HGck+ZJ0rhx4zRmzJiSx16vV6mpqQGvD8EhPFx68UUTRPydZRVp+lS64lcs1tr3a642BLfE+RulMdKMxWkqbmi7GgSL+nPT1OyRH7RuWb588Qm2y6lVDh2SevSo3LIBDSKNGjVSeHj4Sb0feXl5J/WSSFJ0dLSio6MDWRJCQPPmZ1jg8vbSuzPV8cIiKTKyRmpCkJuzQWrQQOlXNJZO3kcCKnY0TXpEah+xWerY3XY1tcrpdhZPFNDBqlFRUerevbvmz59fbv78+fN12WWXBXLVqMvat5eKisxpvEBl+C/tXkFPLXBK/jM6GbB6TgJ+aGbMmDH61a9+pczMTPXo0UMzZ87Uzp07NXLkyECvGnWV/+Z369dzOiYqZ+NGqWtX21Ug2CQmSikpXM35HAU8iNx66606cOCAHn/8ce3Zs0cdO3bUxx9/rBYtWgR61airGjc25/3SOKAyfD4TRG67zXYlCEbp6bQ156hGBqvec889uueee2piVYDpXudS76isXbvMuYZcQwRnIz1dWrjQdhVBLTjuNQNUFUEEleU/vs9hPJyN9u3NlZyLimxXErQIIghN6elmA1NcbLsS1HYbNpgL1bRsabsSBKP0dOn4cWnLFtuVBC2CCEJT+/bmxnc7dtiuBLXdxo3mjrvh4bYrQTDyD45nnMhZI4ggNJU9cwY4Hf+pu8DZ8A+Op605awQRhKbzz5cSEmgccGYbNjA+BGfPPzieHpGzRhBBaHK5zF4uQQSnc/CgtHcvQQTnhrbmnBBEELrYS8GZcMYMqgOD488JQQShy38Kr+PYrgS11bffSmFhjBHBuWFw/DkhiCB0tW9vbgH5ww+2K0FttXq1OWMmNtZ2JQhm/iBLD+xZIYggdPkbB47d4lTWrJE6d7ZdBYJdaqpUr560bp3tSoISQQShq0ULs6fLXgoq4jimR6RLF9uVINi5XCbQ5uTYriQoEUQQusLDzSBEekRQkdxcyeOhRwTVo1s3adUq21UEJYIIQhv3nMGprF5tvtIjguqQkSFt2mTGpaFKCCIIbf7z+zlzBidas0aqX19q1sx2JQgF3bqZdmbNGtuVBB2CCEJb+/bSTz9JeXm2K0Ft4x8f4nLZrgShoH17KTKSwzNngSCC0Naxo/nKIDKcaMUK050OVIeoKKlTJ+mbb2xXEnQIIghtbduaG1ItW2a7EtQmeXnStm3SpZfargShJCODHpGzQBBBaHO5zMaGIIKyvvrKfCWIoDp17y6tXSsdOWK7kqBCEEHou/RSs+Hx+WxXgtpi2TIpOVlq3tx2JQglvXpJx4+XBl1UCkEEoa9HDzNgddMm25WgtvjySxNQGaiK6tShg3TeedLnn9uuJKgQRBD6evQwo9kXLrRdCWqDI0dMEOnTx3YlCDVhYaZXhCBSJQQRhL74eKlnT2n+fNuVoDZYulQqLJSuvtp2JQhFV1wh/fvf5m68qBSCCOqGa66RFi2SiopsVwLb5s+Xzj/fXP4fqG433CAdPSp99pntSoIGQQR1w3XXSfn5Zm8YdZfjSHPnmv8HxocgENLTpdatzf8ZKoUggrqhWzfTOLz5pu1KYNM330jffSfdeqvtShCqXC7pxhulDz+UiottVxMUCCKoG1wuaeBA6R//4NhtXfbmm1KTJgxURWDdfru0e7f06ae2KwkKLsepvXcD83q9crvd8ng8SkxMtF0Ogt3mzWZcwIwZ0ogRZ15+715p8WJz07wDB8x1SBo1kpo2NfeV6NRJatgw4GXXeYWFphdj/XpzCvZPP0mHD0v16kktW0qXXGIuJBV2hv0qr1dq0UIaOlR67rkaKR11WPfupq346KMzL3v0qDlsvHq1tGuX2VlKTDSh+cILza0qWrY88/94LVKV7XdEDdUE2HfBBdL//I80aZI0eLC5N8SJfD5zbPePf5SWLDHzkpPN5HJJ+/ebgOIf9JqcLHXubKZOnczX9HQpOrr663cc00B5PGY6eLDi7z0es9EtLDQXVyo7FRWZ1xERYU5pLjtFRZmNe2KilJBgJv/3iYlmcrvNHWsTE81zVPfr+/FHc2XKnBzTKOfkmPBx/LhZplEjqXFjKS7OvMYdO8zrPP986c47pawss0xFpk41p+6OGVO9dQMVue8+8z/55ZfSZZdVvMzWrdLTT5ueuiNHzOevRQvTfni9pq05dMgsGx9vAom/vfG3OeedF5j6i4rKtyn+Nqaidqeg4OS2pgo9z/SIoG5Zt07q2tU0Es8/Xzq/uFh65x3p8celjRtNw3HXXWYEfOPG5Z+jqMjsoX/7rdlorl1rbv29bZv5eXi4ubX8+eebKSnJbMzj401DEx1tNro+n5mKi80efn7+ydOJH/zCwlO/tsREExLcbvN9VJQJCydO/tdw4lRYaBo9r7d0/ae7Gm18vFnXmabwcPN6JfO1sLB8Q5aXJ23fbqajR0ufu1Mn87fq1MlcKKpDh5NDRkGBtHy59NZb0muvmee/+27pwQfN++63erXpObn7bhMygUArLpYuvth8tr74wnwm/davNztEb71l2pd775V+8QuzE1N2ELXjmEM8J7Y169eXtgWNG5v2plkzKSXFrKdePTPFxZVva3w+8xnLzzef9RPbmrLtzekuUx8XV/4zHhNjdmbKtDNex5H7gw8qtf0miKDumTJF+s1vpCFDpP79pS1bpFdflTZsMMHj0UfNRdCqKj/fNBhr1pg99V27zPTjj+ZDf/iwmY4dM12sYWFmIx0WZj7Y/l6Isj0RZT/s/pBR0fcJCdXfbes4ptHyekunsntIlZ38A/ZcrtLemPr1S19Do0am27lVK/O1QwepTZuqv579+6UXXjB/36Ii6Y47zGnbe/dKTz5pgsl//iPFxlbr2wSc0urV5roibdtK999vegr+/nfpk0+k1FRp7FhzqLCq/5NFReZQ85o1plflhx/MtHt3acjwtzf+tiYszHz+YmLKtzX+qTLtjP/7yMgzlliV7TdBBHWP40h/+Yv08MNm7EdUlPTzn0sPPWT2mhHcfvpJevFFEy5zc03Y69dPmjmzfC8JUBNWrDA9cStWmMeZmaYH5Je/rPjwcIggiACV4fOZ3opGjSqV8BFkHMcEzZgY000N2OI40n//a3oD3W7b1dQIBqsClREWZka1IzS5XKceuArUJJeLM+xOI3jOBQIAACGHIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArAlYENm+fbuGDRumVq1aKTY2Vm3atNGECRNUWFgYqFUCAIAgExGoJ964caN8Pp9mzJihtm3b6ttvv9WIESN0+PBhTZ48OVCrBQAAQcTlOI5TUyt79tlnNX36dH3//feVWt7r9crtdsvj8SgxMTHA1QEAgOpQle13wHpEKuLxeNSgQYNT/rygoEAFBQUlj71eb02UBQAALKmxwapbt27VlClTNHLkyFMuk52dLbfbXTKlpqbWVHkAAMCCKgeRiRMnyuVynXZasWJFud/ZvXu3fvazn+nmm2/W8OHDT/nc48aNk8fjKZlyc3Or/ooAAEDQqPIYkf3792v//v2nXaZly5aKiYmRZELIVVddpUsuuUSzZ89WWFjlsw9jRAAACD4BHSPSqFEjNWrUqFLL7tq1S1dddZW6d++uWbNmVSmEAACA0Bewwaq7d+9W79691bx5c02ePFn79u0r+VlycnKgVgsAAIJIwILIvHnztGXLFm3ZskXNmjUr97MaPGMYAADUYgE7VjJkyBA5jlPhBAAAIHGvGQAAYBFBBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1tRIECkoKFDXrl3lcrmUk5NTE6sEAABBoEaCyEMPPaSUlJSaWBUAAAgiAQ8in3zyiebNm6fJkycHelUAACDIRATyyX/88UeNGDFCc+bMUVxc3BmXLygoUEFBQcljr9cbyPIAAIBlAesRcRxHQ4YM0ciRI5WZmVmp38nOzpbb7S6ZUlNTA1UeAACoBaocRCZOnCiXy3XaacWKFZoyZYq8Xq/GjRtX6eceN26cPB5PyZSbm1vV8gAAQBBxOY7jVOUX9u/fr/379592mZYtW+q2227T3Llz5XK5SuYXFxcrPDxcgwYN0l//+tczrsvr9crtdsvj8SgxMbEqZQIAAEuqsv2uchCprJ07d5Yb47F7925dd911+sc//qFLLrlEzZo1O+NzEEQAAAg+Vdl+B2ywavPmzcs9rlevniSpTZs2lQohAAAg9HFlVQAAYE1AT98tq2XLlgrQUSAAABCk6BEBAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANZE2C7gdBzHkSR5vV7LlQAAgMryb7f92/HTqdVB5MCBA5Kk1NRUy5UAAICqys/Pl9vtPu0ytTqINGjQQJK0c+fOM74QnD2v16vU1FTl5uYqMTHRdjkhjfe6ZvA+1wze55oRjO+z4zjKz89XSkrKGZet1UEkLMwMYXG73UHz5gezxMRE3ucawntdM3ifawbvc80Itve5sh0IDFYFAADWEEQAAIA1tTqIREdHa8KECYqOjrZdSkjjfa45vNc1g/e5ZvA+14xQf59dTmXOrQEAAAiAWt0jAgAAQhtBBAAAWEMQAQAA1hBEAACANbU6iEybNk2tWrVSTEyMunfvrs8//9x2SSElOztbF110kRISEtSkSRMNGDBAmzZtsl1WyMvOzpbL5dLo0aNtlxJydu3apdtvv10NGzZUXFycunbtqpUrV9ouK+QcP35cjz76qFq1aqXY2Fi1bt1ajz/+uHw+n+3SgtrSpUt14403KiUlRS6XS3PmzCn3c8dxNHHiRKWkpCg2Nla9e/fWunXr7BRbjWptEHn77bc1evRojR8/XqtWrdLll1+u66+/Xjt37rRdWshYsmSJRo0apWXLlmn+/Pk6fvy4rr32Wh0+fNh2aSHr66+/1syZM9W5c2fbpYScn376ST179lRkZKQ++eQTrV+/Xs8995zq169vu7SQ88wzz+jll1/W1KlTtWHDBv3hD3/Qs88+qylTptguLagdPnxYXbp00dSpUyv8+R/+8Ac9//zzmjp1qr7++mslJyfrmmuuUX5+fg1XWs2cWuriiy92Ro4cWW5eWlqa8/DDD1uqKPTl5eU5kpwlS5bYLiUk5efnO+3atXPmz5/vXHnllU5WVpbtkkLK2LFjnV69etkuo07o16+fM3To0HLzfvGLXzi33367pYpCjyTn/fffL3ns8/mc5ORk5+mnny6Zd+zYMcftdjsvv/yyhQqrT63sESksLNTKlSt17bXXlpt/7bXX6ssvv7RUVejzeDySSm82iOo1atQo9evXT1dffbXtUkLShx9+qMzMTN18881q0qSJMjIy9Morr9guKyT16tVLCxcu1ObNmyVJq1ev1hdffKEbbrjBcmWha9u2bdq7d2+57WJ0dLSuvPLKoN8u1sqb3u3fv1/FxcVKSkoqNz8pKUl79+61VFVocxxHY8aMUa9evdSxY0fb5YScv//97/rmm2/09ddf2y4lZH3//feaPn26xowZo0ceeUTLly/Xb37zG0VHR+uOO+6wXV5IGTt2rDwej9LS0hQeHq7i4mI99dRT+uUvf2m7tJDl3/ZVtF3csWOHjZKqTa0MIn4ul6vcY8dxTpqH6nHvvfdqzZo1+uKLL2yXEnJyc3OVlZWlefPmKSYmxnY5Icvn8ykzM1OTJk2SJGVkZGjdunWaPn06QaSavf3223rjjTf05ptvqkOHDsrJydHo0aOVkpKiwYMH2y4vpIXidrFWBpFGjRopPDz8pN6PvLy8k9Igzt19992nDz/8UEuXLlWzZs1slxNyVq5cqby8PHXv3r1kXnFxsZYuXaqpU6eqoKBA4eHhFisMDU2bNlX79u3LzUtPT9e7775rqaLQ9eCDD+rhhx/WbbfdJknq1KmTduzYoezsbIJIgCQnJ0syPSNNmzYtmR8K28VaOUYkKipK3bt31/z588vNnz9/vi677DJLVYUex3F077336r333tNnn32mVq1a2S4pJPXt21dr165VTk5OyZSZmalBgwYpJyeHEFJNevbsedLp55s3b1aLFi0sVRS6jhw5orCw8puP8PBwTt8NoFatWik5ObncdrGwsFBLliwJ+u1irewRkaQxY8boV7/6lTIzM9WjRw/NnDlTO3fu1MiRI22XFjJGjRqlN998Ux988IESEhJKeqDcbrdiY2MtVxc6EhISThp3Ex8fr4YNGzIepxr99re/1WWXXaZJkybplltu0fLlyzVz5kzNnDnTdmkh58Ybb9RTTz2l5s2bq0OHDlq1apWef/55DR061HZpQe3QoUPasmVLyeNt27YpJydHDRo0UPPmzTV69GhNmjRJ7dq1U7t27TRp0iTFxcVp4MCBFquuBnZP2jm9P/3pT06LFi2cqKgop1u3bpxWWs0kVTjNmjXLdmkhj9N3A2Pu3LlOx44dnejoaCctLc2ZOXOm7ZJCktfrdbKyspzmzZs7MTExTuvWrZ3x48c7BQUFtksLaosWLaqwTR48eLDjOOYU3gkTJjjJyclOdHS0c8UVVzhr1661W3Q1cDmO41jKQAAAoI6rlWNEAABA3UAQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYM3/B89Hd1rSe47wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved as 'Lab4_basic_regression.jpg'\n",
      "Animation saved as 'Lab4_basic_regression.gif'\n"
     ]
    }
   ],
   "source": [
    "save_final_result(model, x_train, y_train)\n",
    "animate_training(history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX-kDur7xKOW"
   },
   "source": [
    "## Part 3: Binary classification (10%)\n",
    "\n",
    "You will train a model to perform binary classification. Your task is to predict whether an optical coherence tomography (OCT) image shows Choroidal Neovascularization (CNV) or is normal.\n",
    "\n",
    "- Data: OCT scan image of retina\n",
    "- Classes:\n",
    "  - CNV: label = 1\n",
    "  - Normal: label = 0\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 20000 images\n",
    "  - Testing set size: 5000 images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAvUwG1uSLg_"
   },
   "source": [
    "> ### Step 1: Read data & split data\n",
    "Load *basic_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:51:52.564859Z",
     "start_time": "2024-11-05T13:46:41.521923Z"
    },
    "id": "Hp8M93z7v_lO"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'basic_data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasic_data.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\I2ML\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'basic_data.npz'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.load('basic_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names_binary = {0: 'Normal', 1: 'CNV'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "  plt.subplot(330 + 1 + i)\n",
    "  plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "  plt.title(f'Label: {int(Y_train[i])} ({class_names_binary[int(Y_train[i])]})', fontsize=8)\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "# Reshape Y_train to 2D array\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot data distribution\n",
    "Y_train_1 = np.sum(Y_train == 1)\n",
    "Y_train_0 = np.sum(Y_train == 0)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar([0, 1], [Y_train_0, Y_train_1])\n",
    "plt.title('Data distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print('Train: x=%s, y=%s' % (X_train.shape, Y_train.shape))\n",
    "print('Test: x=%s' % (X_test.shape, ))\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=(1 - split_ratio), random_state=42)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r01QzzHxeMbR"
   },
   "source": [
    "> ### Step 2: Training and Evaluation\n",
    "Train your model on the prepared OCT image data and evaluate its performance in distinguishing between CNV and normal retinal conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T13:51:52.564924Z",
     "start_time": "2024-11-05T13:46:41.772348Z"
    },
    "id": "fI7JY5ESjhZ2"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[300], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# You might need to use mini_batch to reduce training time in this part\u001b[39;00m\n\u001b[1;32m     12\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model, losses, history \u001b[38;5;241m=\u001b[39m train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Plot the loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[285], line 6\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, units, activation_functions, loss_function)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_functions \u001b[38;5;241m=\u001b[39m activation_functions\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function \u001b[38;5;241m=\u001b[39m loss_function\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[285], line 25\u001b[0m, in \u001b[0;36mModel.initialize_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m []        \u001b[38;5;66;03m# Store all Dense layers (weights & biases)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m []    \u001b[38;5;66;03m# Store all activation function layers\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     26\u001b[0m   dense \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], i)\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mappend(dense)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = None\n",
    "layers_dims = None\n",
    "activation_fn = None\n",
    "learning_rate = None\n",
    "num_iterations = None\n",
    "print_loss = True\n",
    "print_freq = 200\n",
    "decrease_freq = None\n",
    "decrease_proportion = None\n",
    "# You might need to use mini_batch to reduce training time in this part\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8q0a20XcPtk"
   },
   "outputs": [],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqtnepD-6I20"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_basic.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mERo3g41zsyX"
   },
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "  'ID': range(len(pred_test)),\n",
    "  'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_basic.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_basic.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMCpPFMVdj36"
   },
   "source": [
    "# **Advanced Part (30%)**\n",
    "\n",
    "You will train a model to perform multi-class classification on medical imaging data. Your task is to classify optical coherence tomography (OCT) images of retinal conditions into four different categories.\n",
    "\n",
    "- Data: OCT scan images of retina\n",
    "- Classes:\n",
    "  - CNV (Choroidal Neovascularization): label = 0\n",
    "  - DME (Diabetic Macular Edema): label = 1\n",
    "  - Drusen: label = 2\n",
    "  - Normal: label = 3\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 37754 images\n",
    "  - Testing set size: 6997 images\n",
    "\n",
    "**Notes:** You can implement other functions to improve your rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_GQ3uO128OC"
   },
   "source": [
    "## Step 1: Read data & split data\n",
    "\n",
    "Load *advanced_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVSfqnXqXGdC"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('advanced_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "print(f'Initial shapes:')\n",
    "print(f'Train: X={X_train.shape}, Y={Y_train.shape}')\n",
    "print(f'Test: X={X_test.shape}')\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names = {0: 'CNV', 1: 'DME', 2: 'Drusen', 3: 'Normal'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "  plt.subplot(330 + 1 + i)\n",
    "  plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "  plt.title(f'Label: {int(Y_train[i])} ({class_names[int(Y_train[i])]})', fontsize=8)\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 4  # OCT has 4 classes\n",
    "Y_train = None\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = None\n",
    "X_test = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of Y_train:\", Y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "\n",
    "# Plot class distribution before splitting\n",
    "orig_labels = np.argmax(Y_train, axis=1)\n",
    "unique, counts = np.unique(orig_labels, return_counts=True)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar(unique, counts)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = None\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train = None\n",
    "y_train = None\n",
    "x_val = None\n",
    "y_val = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngmUDGN13ADi"
   },
   "source": [
    "## Step 2: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIi1A-1dFY0u"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = None\n",
    "layers_dims = None\n",
    "activation_fn = None\n",
    "learning_rate = None\n",
    "num_iterations = None\n",
    "print_loss = True\n",
    "print_freq = 50\n",
    "decrease_freq = None\n",
    "decrease_proportion = None\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehjcfSU2XD3-"
   },
   "outputs": [],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXGnS3HQeNUc"
   },
   "source": [
    "## Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_advanced.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHFDuq2BQ2qI"
   },
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "  'ID': range(len(pred_test)),\n",
    "  'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_advanced.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_advanced.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J91ff4Vk1oB_"
   },
   "source": [
    "# Save outputs\n",
    "Save the outputs of your testing codes to: *Lab4_output.npy*\n",
    "\n",
    "We will test your *Lab4_output.npy* to verify the correctness of your neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpxmIFiW1tg9"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert list(outputs.keys()) == [\n",
    "  'dense_forward',\n",
    "  'dense_backward',\n",
    "  'dense_update_parameters',\n",
    "  'sigmoid',\n",
    "  'relu',\n",
    "  'softmax',\n",
    "  'linear',\n",
    "  'sigmoid_backward',\n",
    "  'relu_backward',\n",
    "  'softmax_backward',\n",
    "  'linear_backward',\n",
    "  'model_forward_sigmoid',\n",
    "  'model_forward_relu',\n",
    "  'model_forward_softmax',\n",
    "  'model_backward_sigmoid',\n",
    "  'model_backward_relu',\n",
    "  'model_update_parameters',\n",
    "  'compute_BCE_loss',\n",
    "  'compute_CCE_loss'\n",
    "], \"You're missing something, please restart the kernel and run the code from beginning to the end. If the same error occurs, maybe you deleted some outputs, check the template to find the missing parts!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDqCzhsp1yTb"
   },
   "outputs": [],
   "source": [
    "np.save(\"Lab4_output.npy\", outputs)\n",
    "\n",
    "# sanity check for saved outputs\n",
    "submit = np.load(\"Lab4_output.npy\", allow_pickle=True).item()\n",
    "for key, value in submit.items():\n",
    "  print(f\"{key}: {type(value)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "I2ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
